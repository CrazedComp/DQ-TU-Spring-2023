\documentclass[12pt]{book}
\usepackage[width=4.375in, height=7.0in, top=1.0in, papersize={5.5in,8.5in}]{geometry}
\usepackage[pdftex]{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tipa}
%\usepackage{txfonts}
\usepackage{textcomp}
%\usepackage{array}
%\usepackage{xy}
\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{cancel}
\usepackage{graphics}
\usepackage{caption}

\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\fancyhf{}
\fancyhead[LE,RO]{\bfseries\thepage}
\fancyhead[LO]{\bfseries\rightmark}
\fancyhead[RE]{\bfseries\leftmark}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0pt}
\addtolength{\headheight}{0.5pt}
\setlength{\footskip}{0in}
\renewcommand{\footruleskip}{0pt}
\fancypagestyle{plain}{%
\fancyhead{}
\renewcommand{\headrulewidth}{0pt}
}
%
%\parindent 0in
\parskip 0.05in

\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta
	urlcolor=cyan
	urlstyle{same}
	}
%\href{www.aaa.com}{This Link}

\usepackage{amsthm}
%\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%\newenvironment{example}[2][Example]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%\newenvironment{Recall}[2][Recall]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%\newenvironment{remark}[2][Remark]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%\newenvironment{definition}[2][Definition]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

%\theoremstyle{definition}
%\newtheorem{definition}{Definition}
%\newtheorem*{definition*}{Definition}
%\newtheorem{remark}{Remark}
%\newtheorem*{remark*}{Remark}
%\newtheorem{example}{Example}
%\newtheorem*{example*}{Example}
%\newtheorem{exercise}{Exercise}
%\newtheorem*{exercise*}{Exercise}
%
%\theoremstyle{plain}
%\newtheorem{theorem}{Theorem}
%\newtheorem*{theorem*}{Theorem}
%\newtheorem{lemma}{Lemma}
%\newtheorem*{lemma*}{Lemma}
%\newtheorem{corollary}{Corollary}
%\newtheorem*{corollary*}{Corollary}
%
%\theoremstyle{proof}
%\newtheorem{asd}{Exercise}
%\newtheorem*{exercise*}{Exercise}




\usepackage{fancyvrb} 
%\begin{Verbatim}[numbers=left, frame=single, formatcom=\color{red}]

\begin{document}
\frontmatter
%
\chapter*{\Huge \center Probability Theory I Notes}
\thispagestyle{empty}
%{\hspace{0.25in} \includegraphics{./ru_sun.jpg} }
\section*{\huge \center Dean Quach}



\newpage
\section*{\center \normalsize Course Info}
\subsection*{\center \normalsize Probability Theory I}

\begin{center}
Janurary 17, 2023 to May 9, 2023\\
MWF --- 1:00 pm - 1:50 pm --- Wachman 407\\
Prof. Mr. Wei-Shih Yang\\
CRN 37229

3 credits $\implies{}$ 6 hours of homework
\end{center}

\subsection*{\center \normalsize Text}
\begin{center}
Introduction to Probability Theory
\begin{itemize}
\item David F. Anderson
\item Timo Sappalainen
\item Benedek Valko
\end{itemize}
Edition 1
\end{center}


\newpage
\section*{\center \normalsize Other Notes}
\begin{center}
2 Midterms $\implies 20\%$ each\\
Final $\implies 35\%$\\

Quizzes $\implies 12\%$ each\\
(every other Friday)\\

HW $\implies 3\%$ each\\
(due every Friday)\\
\end{center}

\vspace{50pt}

\begin{center}
Office Hours: Wachman 534\\
MWF 3:00 pm - 4:00 pm\\
Phone for texting: \textit{removed, because I'm not stupid}\\%(215) 983 - 3816\\
\end{center}


%
\chapter*{\center \normalsize Spring 2023\\
Math 3031\\
Section 001\\}
%











\chapter*{Preface}
\hspace{\parindent} Those of you who aren't familiar with me, I am just a below average math major. Struggling as always; in his 4th semester at Temple University (Spring 2023). I had originally wrote this for some close friends that I had met along the way, mainly one which had introduced me to a bunch of his other friends which had introduced me to more, like induction or whatever :) 

\begin{center}First I'd like to thank you for looking at my first book. \\Use it for whatever you need.\end{center}

Whether this be an aid to if your sick and you can \\``borrow" my notes like I'm your friend\footnote{its why I typed them up, since my handwritting is cursive. I.E. It's beautiful, but illegible to most.}, or this may be a way for you to see what an undergraduate math course looks like at a (what I would call) normal university. 
Worst case for me, but best case for you, this could be something that is hilarious to you, not only because of my footnotes, but because you might be; a graduate student, an actual adult (with a full time job), one of my professors (oh dear god), someone I dislike (please no)...a relative or friend (it can't get worse)...\scriptsize or my future self... \normalsize \\
\textbf{No! God!...No God, please no, No!...NO!...NOOOOO!!!} --- Michael Scott, The Office.

Now, those friends know who they are, and have helped me feel comfortable adopting the phrase I say a lot, which is ``it's like `math major' is my whole fricking personality".
They did not directly sit down with me, hold a weapon to my head, and made me believe that by force... Rather they had asked me many (and I mean many) questions over 2 years (at this point). 
Along with doing my research project on the Perfect Cuboid (since my GPA at the time isn't great due to my laziness, so of course I could not get other academic jobs), I feel as if everyone close around me is aware that they are finally helping me break out of my imposter syndrome of being a mathmetician. 
I certainly believe I will never break out of the imposter syndrome, I definetely do not believe I deserve to be here, let alone have the ``right" to call myself a mathmetician (this feels very wrong for me to say, even type). 
To me I still feel like I am just a teenager (even though I'm 20 as of writting this), just doing math problems for fun, and equally butchering my quizzes and tests\footnote{Ironically, these close friends I am talking about, have no idea what I am talking about. I am refering to other math classes that kicked my ass. But in the classes I took with them (such as this class and Calc 3), they always saw me get good grades with 0 sleep and 80\% caffienated blood. Hilariously stupid on my end.}. 

I hope this helps as another way someone relates to you, but enough about me and more about the book. These were meant to be my personal notes. 
I had originally started typing them up in Obsidian, which is a greate program with ``atomic" style linking. 
But even though it had \LaTeX ~(actually MathJax) and other coding implements, I found myself just linking in a string all the lectures together. 
Sometimes the atomic style worked, but it more or less became a thing I had to organized, and although I could have set it up with a tutorial on how to make it future proof, I believe it made more sense for me to go back to \LaTeX ~and just comment out what lecture \# it is, and basically write my notes like a book.

I don't know if it is obvious by know, but these, as I said, were meant to be my personal notes. 
I had actually only meant to share it to those close friends I talked about on my GitHub.
Anyways, much of this is literally based off of the textbook my course used. 
Also it wasn't meant to be a book, but I had realized about page 73 (the week of my midterm and me procrastinating as always), that I am basically just writting a book. 
At this point I had the cool idea that I wanted to print this out for my personal collection, at least I think it'll be pretty cool to have a bunch of books made with ``no additional effort", that are technically mine. 
And then I could possibly go back and rewrite my old notes for other courses, and the new courses will be an easy case. 
And then I realized I could probably just publish this. So I hope, like in paragraph 2, that you find this useful or funny, hopefully both.
\footnote{Also I hope that this is the first book you read out of all of my (hopefully) upcomming books. Since this is my first book, and therefore, this is also the first preface (which should give a good insight to who I am and how I write).} 
Thank you for reading!

\begin{center}
\textbf{Disclaimer: Some things I write may be completely wrong. Usually the arithmetic. This is a personal journal, NOT an official study guide/aid. If you fail because of me, (ha!) but I'm not responsible. Go study for real instead of reading this book. Actually no, please read this book to, you might find some interesting things. Also please contact me at deanquach.com if theres any problems. I do want to fix any problems and I'll submit new editions when enough are caught. On the mean time, you should be looking at this from my GitHub. You can message message me there instead.} 
\end{center}
%Last sentence of disclaimer eventually: Since this is published, if something seems wrong, my website will also have the errata list for this edition, thanks to the people that reached out to me.








%
\tableofcontents
%
\mainmatter
%














%%%-------------- Lecture 1: 1/18/23
\chapter{Experiments with random outcomes}
\section{Sample Spaces \& Probability}
\textbf{Definition (Sample Space).}  The set of all possible outcomes of an expirement is called a sample space, \\denoted by $\Omega$.\\

\noindent \textbf{Definition.} A subset of a sample space $\Omega$ is called an ``Event". 

\subsection{The Sample Spaces}
We first look at the notion of Sample Spaces through some examples.\\
\textbf{E.g. 1.} Tossing a coin.
\begin{itemize}
\item$\Omega=\left\{ H,T \right\}$.
\item$A=\{H\}\subseteq \Omega$.
\item$B=\{T\}\subseteq \Omega$.
\item Also by definition, $\emptyset \subseteq \Omega$.
\begin{itemize}
\item This is by definition of a set, but in real life:\\``It balances on the edge"\footnote{Although you may say, but theres a ``chance" it lands on it. The main idea is we weren't measuring/counting that possibility. As in with measure theory. But if you may, think of a table, we were only filling out $H$ or $T$, and $on~the~edge$ isn't even an option. Therefore implying 0 probability.}, ``lost the coin", \\``coin rolled away".
\end{itemize}
\item $\Omega \subseteq \Omega$.
\begin{itemize}
\item With $P(\Omega)=1$, or the probability that ``heads or tails" shows up. 
\end{itemize}
\end{itemize}

\noindent\textbf{E.g. 2.} Rolling a die. (assume 6-sided)
Some examples of events include, 
\begin{itemize}
\item $A = \{ even~\#'s \} = \{ 2,4,6 \}$
\item $B = \{ 1,5 \}$
\item $\emptyset \implies P(\emptyset) = 0$
\item$\Omega \implies P(\Omega) = 1$
\end{itemize}
and all events are subsets within a sample space,\\
i.e. $\{ A,B,\emptyset, \Omega \} \subseteq \Omega$.\\

\noindent Notice, $P(\emptyset)=0$ always. \\
Also notice, $``\subseteq" \implies{}$ proper subset. So this is why $\Omega$ is an event itself.

\noindent \textbf{Question:} How many posibilities?\\
Well you can think of each option within a simple sample space as being on or off, a binary choice ${1 \lor 0}$. 
In the mindset of ``Measure Theory", you can treat all these posibilities/probabilities as measures of a sample space, i.e. asking for ``all the posibilities" $\implies{}$ ``all the subsets" $\implies{}$ 
``the cardinality of the (power set)"$:=\mathbb{P}$\footnote{The reason why I will use $\mathbb{P}$ for power set is because in probability theory, $P()$ is already used to describe probability. But for the rest of my notes, I will just specify each time I use it (just in case). I may end up keeping this notation from now on; I will link back to this footnote if need be. Since sometimes its easier to just type $P()$ and also $\mathbb{P}$ is used for polynomial vector space in Theoretical Linear Algebra, but I'll just stop the rant and say "If in this course 3031 notes, and I didn't link back to this, you know what I meant". OH!, also I forgot, but another way is $\mathbb{P}() \implies 2^{\Omega}$, without cardinality symbol, thats the best work around I'll probably also use.
} of the sample space" $=2^{|\Omega|}=2^6=64$, where in this instance ``$||$" asks for the cardinality of a set. 
So we have a new idea, \\

\noindent \textbf{Idea (Event Space)} The \textit{class} (set) of \textit{all events} of a sample space $\Omega$ is denoted by $\mathcal{F}$.\\

This in itself can be a sample space or an event space, but this is more or less just an informal definition of what an event space should be.

But note, this is for a sample space of $\Omega = \{ 1,2,3,4,...n \}$, noting in real life, rolling a die you can only have \textbf{1 face up}, and therefore $\mathcal{F}= \{ \{1\}, \{2\}, \{3\}, \{4\}, \{5\}, \{6\} \}$.\\

\noindent \textbf{E.g. 2 (continued).} Now for Rolling a die. (assume 6-sided), what is $\mathcal{F}$?

By hand/written out, it would be 
$$\mathcal{F}= \Big\{ ~\emptyset,$$
$$\{1\}, \{2\}, \{3\}, \{4\}, \{5\}, \{6\},$$
$$~$$
$$\{1,2\}, \{1,3\}, \{1,4\}, \{1,5\}, \{1,6\}, $$ 
$$\{2,3\}, \{2,4\}, \{2,5\}, \{2,6\},  $$
$$\{3,4\}, \{3,5\}, \{3,6\},  $$
$$\{4,5\}, \{4,6\},  $$
$$\{5,6\},  $$
$$~$$
$$\{1,2,3\}, \{1,2,4\}, \{1,2,5\}, \{1,2,6\},  $$
$$\{1,3,4\}, \{1,3,5\}, \{1,3,6\}, $$
$$\{1,4,5\}, \{1,4,6\}, $$
$$\{1,5,6\},$$
$$\{2,3,4\}, \{2,3,5\},  \{2,3,6\},  $$
$$\{2,4,5\}, \{2,4,6\},$$
$$\{2,4,6\},$$
$$\{3,4,5\}, \{3,4,6\},$$
$$\{3,5,6\}, $$
$$\{4,5,6\},$$
$$~$$
$$\{1,2,3,4\}, \{1,2,3,5\}, \{1,2,3,6\}$$
$$\{1,2,4,5\}, \{1,2,4,6\},$$
$$\{1,2,5,6\},$$
$$\{1,3,4,5\}, \{1,3,4,6\},$$
$$\{1,3,5,6\},$$
$$\{1,4,5,6\},$$
$$\{2,3,4,5\}, \{2,3,4,6\}, $$
$$\{2,3,5,6\},  $$
$$\{2,4,5,6\}, $$
$$\{3,4,5,6\},$$
$$~$$
$$\{1,2,3,4,5\}, \{1,2,3,4,6\}, $$
$$\{1,2,3,5,6\}, $$
$$\{1,2,4,5,6\}, $$
$$\{1,3,4,5,6\}, $$
$$\{2,3,4,5,6\},$$
$$\Omega ~\Big\}.$$

\noindent Which has $|\mathcal{F}| = 2^{|\Omega|} = 2^{36}$ events (subsets).\\

Although we can find how many events are in the event space, we don't have a practical way to list them all out. Also This is not generalizable, because sometimes we are looking at only a few of the events, such as drawing 3 dice out of an urn with 6 die. For this we turn to...

\subsection{The Event Space}

\textbf{Definition (Event Space).} The collection of all events is called the ``Event Space", denoted by $\mathcal{F}$ (or any calligraphic letter such as $\mathcal{A}$)

Now with the example earlier, we may write more efficiently
$$\mathcal{F}=\Big\{ \{i_1, i_2, i_3, ..., i_k\} ~\Big|~ i_1, i_2, i_3, ..., i_k\in\Omega,$$
$$\forall~ i_j \neq i_k ~ \text{``distinct"}, ~ for ~ k=0,1,2,3,4,5,6~ \Big\}.$$

\noindent \textit{Remark.} This definition of $\mathcal{F}$, is far more concrete since it considers when were not looking for all subsets. 
i.e. If you tossed one die, then there are 6 sample points, but also only 6 events (the event that you toss a die and obtain 4 numbers on the top shouldn't be considered). Also look at Ex 1.1 from the Homework as an example of this, noticing $\mathcal{F}$ is obviously not $2^{|\Omega|}=2^{36}$, rather it is just $6*\dot6$ since there are 2 dice with only 1 face from each as an event.\footnote{In fact later we will look at how to count the number of possible events.}

Now, this efficient definition of $\mathcal{F}$ yields the idea of a probability measure $P$ or $\mathbb{P}$.

\noindent \textbf{Definition (Probability Measure).} Let $\Omega$ be a sample space and $\mathcal{F}$ be the set of all events of $\Omega$. 
A ``probability measure" is a function $P: \mathcal{F} \rightarrow \mathbb{R},~s.t.$
\begin{itemize}
\item[(i)] $0\leq P(A)\leq 1,~\forall~ A\subseteq \Omega$
\item[(ii)] $P(\Omega) =1 ~\text{and}~ P(\emptyset) =0$
\item[\hypertarget{axiom(iii)}{(iii)}] Let $A_1, A_2, ...$ be a sequence of pairwise disjoint events. Then 
\end{itemize}
$$P\left( \bigcup\limits_{i=1}^{\infty} A_i\right) = \sum\limits_{i=1}^{\infty}P(A_i)$$
\noindent Where $A$ is an event in the event space $\mathcal{F}$, or just $A\in\mathcal{F}$.

\subsection{The Probability Space}
Important! \\
\noindent \textbf{Definition (Probability Space).} The triple $(\Omega,\mathcal{F}, P)$ is called a probability space. \footnote{Note, $A_1, A_2, ...$ are said to be pairwise disjoint if $A_i \cap A_j = \emptyset,~\forall~ i\neq j$.}\\

\noindent\textbf{Remark.} This is for infinite measures (since we are definind a $\sigma$-algebra). But for finite sequence we have the following theorem.\\

\noindent \textbf{Theorem 1.2.} \textit{Let $(\Omega, \mathcal{F}, P)$ be a probability space. Let $A_1, A_2, ...A_n$ be a finite sequence of pair pairwise disjoint events. Then 
$$P\left( \bigcup\limits_{i=1}^{n} A_i\right) = \sum\limits_{i=1}^{n}P(A_i).$$}

Basically, this is saying, $\dot{\bigcup}A_i$ for $\infty$ many $\implies{}$ $\dot{\bigcup}A_n$ works for finitely many. Seems doable and reasonable, so...\\

\noindent\textit{Proof:} Let $A_1, A_2,...,A_n$ be pairwise disjoint events. 
Now, we need to set the ``rest" of the events to have probability 0, so we are setting $\infty$ many of them.
Set $A_{n+1} = A_{n+1} =\cdots = \emptyset$. \\
Then $A_1 \bigcup \cdots \bigcup A_n=A_1 ~\dot{\bigcup}~ \cdots ~\dot{\bigcup}~A_n ~~~\dot{\bigcup}~ A_{n+1} ~\dot{\bigcup}~ A_{n+2} ~\dot{\bigcup}~ \cdots$\\

\noindent By axiom \hyperlink{axiom(iii)}{$(iii)$}, we have $P\left( \bigcup\limits_{i=1}^{n} A_i\right) = P\left( \bigcup\limits_{i=1}^{\infty} A_i\right) = \sum\limits_{i=1}^{\infty}P(A_i)$ 
$= \left(\sum\limits_{i=1}^{\infty}P(A_i) \right)+0+0+0+\cdots = \sum\limits_{i=1}^{n}P(A_i)$.\\
$$\therefore P\left( \bigcup\limits_{i=1}^{n} A_i\right) = \sum\limits_{i=1}^{n}P(A_i)~.$$\hfill$q.e.d.$


\subsection{Introducing Cartesian Products}
Let $A,B$ be sets. Then
$$A \times B = \Big\{  (a,b) ~\Big|~ a\in A, ~ b\in B \Big\}.$$

\noindent\textbf{E.g.} Toss a coin \& Roll a die. What is the sample space $\Omega$?

Using what we call a ``Tree Diagram"...
\begin{center}
\begin{tikzpicture}
\node{Sample Space}
[level distance=1cm]
[sibling distance=.5cm]
	child {node{heads}
		child{node{1}}
		child{node{2}}
		child{node{3}}
		child{node{4}}
		child{node{5}}
		child{node{6}}}
child[missing]
child[missing]
child[missing]
child[missing]
child[missing]
child[missing]
child[missing]
	child {node{tails}
		child{node{1}}
		child{node{2}}
		child{node{3}}
		child{node{4}}
		child{node{5}}
		child{node{6}}};
\end{tikzpicture}
\end{center}

\noindent or alternatively

\begin{center}
\begin{tikzpicture}
\node{Sample Space}
[level distance=1cm]
[sibling distance=.5cm]
	child {node{1}
		child{node{H}}
		child{node{T}}}
child[missing]
	child {node{2}
		child{node{H}}
		child{node{T}}}
child[missing]
	child {node{3}
		child{node{H}}
		child{node{T}}}
child[missing]
	child {node{4}
		child{node{H}}
		child{node{T}}}
child[missing]
	child {node{5}
		child{node{H}}
		child{node{T}}}
child[missing]
	child {node{6}
		child{node{H}}
		child{node{T}}};
\end{tikzpicture}
\end{center}

\noindent either way, they both imply
$$\Omega = \Big\{  (H,1), (H,2), (H,3), (H,4), (H,5), (H,6),$$
$$  (T,1), (T,2), (T,3), (T,4), (T,5), (T,6), \Big\}  $$

Or by definition of cartesian products, we can say, \\$~~\Omega = \{H,T\} \times \{ 1,2,3,4,5,6 \}$.\footnote{Recall: The definition of cartisian product is that its an ordered pair, not an unordered set. So our graph must reflect that.} Which we know we can graph, at least for cartisian products of 3 or less sets \\(usually only for 2).



\begin{center}
\begin{tikzpicture}
\draw[->] (-1,0) -- (6,0) 
node[right] {$Coin~Toss$}; 
\draw[->] (0,-1) -- (0,7) 
node[above] {$Die~Toss$}; 

\foreach \y in {1,2,3,4,5,6}
	\draw [shift={(0,\y)}, color=black] (2pt,0pt) -- (-2pt,0pt) node[left] {$\y$};

\draw [shift={(2,0)}, color=black] (0pt,2pt) -- (0pt,-2pt) node [below] {$H$};
\draw [shift={(4,0)}, color=black] (0pt,2pt) -- (0pt,-2pt) node [below] {$T$};

\coordinate (A) at (2,1);
\filldraw (A) circle (2pt);

\coordinate (A) at (2,2);
\filldraw (A) circle (2pt);

\coordinate (A) at (2,3);
\filldraw (A) circle (2pt);

\coordinate (A) at (2,4);
\filldraw (A) circle (2pt);

\coordinate (A) at (2,5);
\filldraw (A) circle (2pt);

\coordinate (A) at (2,6);
\filldraw (A) circle (2pt);

\coordinate (A) at (4,1);
\filldraw (A) circle (2pt);

\coordinate (A) at (4,2);
\filldraw (A) circle (2pt);

\coordinate (A) at (4,3);
\filldraw (A) circle (2pt) node[right] {$(T,3)$};

\coordinate (A) at (4,4);
\filldraw (A) circle (2pt);

\coordinate (A) at (4,5);
\filldraw (A) circle (2pt);

\coordinate (A) at (4,6);
\filldraw (A) circle (2pt);
\end{tikzpicture}
\end{center}








\subsection{Set Theory Examples}
%%%------------------- Lecture 2: 1/20/23
\textbf{Exercise B.4.} Show that identity the ``de Morgan" Law, $$\left( \bigcap\limits_i A_i\right)^{C}=\bigcup\limits_i A_i^{C},~~~\text{ is true.}$$\\

\noindent\textit{Recall:} We need to show that the LHS and RHS are subsets of eachother.\\ 
Similar to ``if and only if", however instead of \textbf{saying} 
\begin{itemize}
\item ``if $\rightarrow$ then... 
\item Conversely $\leftarrow$ then... 
\item Therefore $\iff{}$..." 
\end{itemize}

\noindent For set theory, we would \textbf{check} (for proper subsets btw), 
\begin{itemize}
\item $\subseteq :~~~$``Let $a \in A$, check that $a \in B$... yes! So since $a$ started in $A$, but is also in $B$, hence $A\subseteq B$"
\item $\supseteq :~~~$``\textbf{Now} (not conversely), let $b \in B$, check that $b \in B$... yes! So since $b$ started in $B$, but is also in $A$, hence $B\subseteq A$"
\item $=: ~~~$ Therefore since $\subseteq$ and $\supseteq$, then $A=B$.
\end{itemize}
Now that we remember this,\\

\noindent \textit{Proof by Professor:} ``$\subseteq$" direction first.

\noindent Let $x \in \left( \bigcap\limits_i A_i\right)^{C}$. \\

$\implies x \notin A_i=\{y~|~ y\in A_i,~\forall i\}$.\\

\noindent Then $\exists i~s.t.~ x\notin A_i$.\\
So $x \in \bigcup\limits_i A_i^{C}$\\
Hence, $\left( \bigcap\limits_i A_i\right)^{C}   \subseteq  \bigcup\limits_i A_i^{C}.$\\

\noindent``$\subseteq$" direction now.\\

\noindent Now let $x \in \bigcup\limits_i A_i^{C}$. \\
Then $x \in A_i^{C}$, for some $i\in \mathbb{Z}$.\\
Then $x \notin A_i$, for some $i\in \mathbb{Z}$.\\
Then $x \notin \bigcap\limits_i A_i \implies x\in \left( \bigcap\limits_i A_i\right)^{C}$,\\
Hence, $\bigcup\limits_i A_i^{C} \subseteq \left( \bigcap\limits_i A_i\right)^{C}$.\\

Therefore, \\
$$\left( \bigcap\limits_i A_i\right)^{C}=\bigcup\limits_i A_i^{C}$$\hfill$q.e.d.$\\


\noindent \textbf{Exercise B.5.} If $A$ and $B$ are sets then let $A triangle B$ denote their symmetric difference: the set of elements that are in exactly one of the two sets. (In words this is “$A$ or $B$, but not both.”) Prove that the symmetric difference is associative.\\

\noindent \textit{Solution not Proof :)} \\
\indent We then go over B.5. however he goes over it with a proof by venn diagram. \textit{Refer to Fig. 1.} Which is somewhat trivial, and does not require much explanation/studying (which is why I dont really consider it a proof). 
However this is similar to doing proofs by table and I dislike this. But this is one of the few times I'll say, the proof is left as an exercise to the reader.\begin{figure}[h]
\begin{center}
\includegraphics[width=2in]{1-20-23 Venn Diagram Proof of B.5.jpg}
\caption*{Fig. 1}
\end{center}
\end{figure}\footnote{I am aware (and not naive...I think), that prooving this properly would take ~10 pages and 5 hours (not actually). Also yess thats my handwritting, see its not that bad, kind of...}

Actually, since I'm stubborn $+$ I'm a nerd $=$ Hard way
Both are direct proofs.

\noindent \textit{Proof by Internet}\\
Similar to the venn diagram, we know\\
$(A \triangle B) \triangle C = (A \cup B \cup C) \cap (A \cup B^{C} \cup C^{C}) \cap (A^{C} \cup B \cup C^{C}) \cap (A^{C} \cup B^{C} \cup C)$\\
$= (B \cup C \cup A) \cap (B \cup C^{C} \cup A^{C}) \cap (B^{C} \cup C \cup A^{C}) \cap (B^{C} \cup C^{C} \cup A)$\\
$=(B \triangle C) \triangle A$.\\

Now, $\triangle$ is commutative since by $def^n$, obviously \\
$A\triangle B=(A\cap B^{C}) \cup (B \cap A^{C})= (B \cap A^{C}) \cup (A\cap B^{C}) = B\triangle A$.\\
\noindent So $(B \triangle C) \triangle A = A \triangle (B \triangle C)$.\hfill$q.e.d.$\\

\noindent \textit{Proof by Stubborness}\\
\noindent Notice $A\triangle B=(A\cap B^{C}) \cup (B \cap A^{C})$.\\

\noindent Then \\
$(A \triangle B) \triangle C = ((A \triangle B)\cap C^{C}) \cup (C \cap (A \triangle B)^{C})$\\
$=(((A\cap B^{C}) \cup (B \cap A^{C}))\cap C^{C}) \cup (C \cap ((A\cap B^{C}) \cup (B \cap A^{C}))^{C})$\\

\noindent Recall the De Morgan's law, $(A \cap B)^{C}= A^{C} \cup B^{C}$ and $(A \cup B)^{C}= A^{C} \cap B^{C}$\\

\noindent So, \\
$(((A\cap B^{C}) \cup (B \cap A^{C}))\cap C^{C}) \cup (C \cap ((A\cap B^{C}) \cup (B \cap A^{C}))^{C})$\\
$=(((A\cap B^{C}) \cup (B \cap A^{C}))\cap C^{C}) \cup (C \cap (A\cap B^{C})^{C} \cap (B \cap A^{C})^{C})$\\
$=(((A\cap B^{C}) \cup (B \cap A^{C}))\cap C^{C}) \cup (C \cap A^f{C} \cup B \cap B^{C} \cup A)$\\

\noindent ... yeah so I give up on this, I'll link a note on this when I find a clever but complete solution. Now that I have officially wasted 30 minutes of my saturday...\qed





\section{Random Sampling }
\subsection{Population and Sample Size}

\textbf{Some notation.}
Simply put, 
\begin{itemize}
\item Population Set: What we've been calling the sample space.
	\begin{itemize}\item``Population Size" $\# \Omega = n$.\end{itemize}
\item Sample Set: What we've been calling an event.
	\begin{itemize}\item``Sample Size" $\# A = k$, where $A$ is an event from the ``event space".\end{itemize}
\end{itemize}

I believe our professor is changing the terminology since it's more used or more practical, and less mathematical with its measure theory roots.

Main takeaways being, sample space $\neq$ sample set. They're different things, not related.

Also, we may still use $\Omega$ for both ``Sample Space" and ``Population Set".
As well as $\mathcal{F}$ for the ``Event Space" and ``Sample Size".
But more efficiently: I've been using the cardinality of these sets to denote the number of elements. But to step away from set theory, the textbook and other places use $\#$ instead.\\

\noindent \textbf{Quick example.}\\
Let the ``Population Set" $\Omega=\{a,b,c,d\}$,\\
then the ``Population Size" $\# \Omega = n = 4$.\\

\noindent Let our ``sample" be this event $A = \{b,d\}$,\\
then the ``sample size" $\# A = k = 2$.\\

\noindent \textbf{How Many Samples are there?}\\
This is our next important question. \\
--- How many samples are there given a population set, \\
or \\
--- rather how many events are there from a sample space.\\

\noindent \textit{Remark.} This question is kind of ambiguous, as I am not asking amount of all events in the ``event space" which would be the full set of all events (i.e. the cardinality of the power set). I am talking about what is the number of all the events if we have restrictions on \textit{how} we sample. 

So what are the restrictions/``procedures" we can do when making an event from a sample space. Well we can,
\begin{enumerate}
\item Sample with replacement, order matters := $n^k$
\item Sampling without replacement, order matters := \\$nPk = \frac{n!}{(n-k)!}$
\item Sample without replacement, order doesn't matter := $nCk = {n \choose k} = \frac{n!}{k!(n-k)!}$
\item Sample with replacement, order doesn't matter := \\${k+n-1 \choose n-1}={k+n-1 \choose k}$ 
\end{enumerate}

All of these are somewhat explanatory, increasing in complexity.

Also similarly, finding out (the amount of events) or (the different ways), we can sample goes from simple to complex as well.

But I'll try to walk through each logically, as well as proof wise, and I'll derive the formulas for finding out the number of possible samples/events.





\subsection{Sampling with replacement, \\order matters}
Suppose we have a sample space with $n$ elemtents. \\
--- I have a bag of $n=15$ marbles. They are colored.\\
Now suppose we want to take a sample of size $k$. \\
--- I want to pick $k=4$ marbles.\\

\noindent Let's choose: \\
We want the order to matter, so we will pick up one at a time, and record it.
\begin{itemize}
\item I grab 1 marble, its red. (then I mark it down)
\end{itemize}
We want the to sample \textbf{with} replacement, so after we pick one, we put it back in the bag.\\
--- I'll put this back in the bag (so theres a chance I could pick it again).

\noindent Then we repeat until we get our full sample (of size $k$).
\begin{itemize}
\item I grab 1 marble, its blue. (I mark it down) Ok, I'll put it back.
\item I grab 1 marble, its red. (I mark it down) Ok, I'll put it back. 
\item I grab 1 marble, its clear. (I mark it down) Ok, I'll put it back.
\end{itemize}
Thats what sampling with replacement looks like. \\
--- Overall: I have a bag of 15 marbles, and my sample was ``I chose 4 marbles". 

In this scenario, Imma put back the marbles each time (so I could get the same one each time). Also in this scenario, I care about the order. \\
--- So this time I got $\omega =$(red, blue, red, clear).\\

\noindent \textbf{How many outcomes are there? Logically.}\\
Well, simply put:\\
The first marble has $n$ posibilities. \\
The second marble has $n$ posibilities.\\
The third marble has $n$ posibilities.\\
The...$~~~~~~~~~~~~~~~~~~~~~$...posibilities.\\
$\downarrow ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\downarrow$\\
The $k$th marble nas $n$ posibilities.

This is because we're sample with replacement, \\
i.e. each time I choose a marble, I put it back in the bag,\\
so I always have 15 marbles in the bag, so 15 posibilities.

So we have $$n*n*n*n*\cdots *n = n^k$$
$$k~~~times$$
And the number of outcomes is $$\#\Omega = n^k~.~~~\square$$

\noindent \textbf{How many outcomes are there? Proof wise.} \\
\textit{Proof.}
Let $S=\{1,2,...,n\}~for~some~n\in \mathbb{N}$.\\
Set an element of our sample space $\omega \in \Omega$ to be an ordered $k$-tuple of elements $s_i \in S.$\\
So $\omega = (s_1, s_2, s_3,..., s_k), ~for~some~k\in \mathbb{N}$.\\
Then 
$$\Omega = \big\{ (s_1, s_2, ..., s_k)_{k\in \mathbb{N}} ~\big|~ s_i \in S, ~i=1,2,...k \big\}$$
but this implies
$$ \Omega = S \times S \times \cdots \times S, ~k~times.$$
So therefore
$$ \#\Omega = \prod\limits_{i=1}^k S_i = n^k,~for~some~k\in\mathbb{N}.\footnote{Also quickly note, the probability measure (probability), if each event/outcome is equally likely implies 
$$P(\omega)=\frac{1}{\#\Omega} = n^{-k}~.~~~\square$$}$$\hfill$q.e.d.$




\subsection{Sampling without replacement,\\ order matters}
Suppose we have a sample space with $n$ elemtents. \\
--- I have a bag of $n=15$ marbles. They are colored.\\
Now suppose we want to take a sample of size $k$. \\
--- I want to pick $k=4$ marbles.\\

\noindent Let's choose: \\
We want the order to matter, so we will pick up one at a time, and record it.
\begin{itemize}
\item I grab 1 marble, its red.
\end{itemize}
This time, we sample \textbf{without} replacement, so after we pick one, we keep it and don't put it in the bag. \\
--- I put this on a rack, now the bag has $n-1=15-1=14$ marbles left.

\noindent Then we repeat until we get our full sample (of size $k$). 
\begin{itemize}
\item I grab 1 marble, its blue. Ok, I'll put it on the 2nd position on my rack. Now theres $n-2=15-2=13$ marbles left.
\item I grab 1 marble, its red. Ok, I'll put it on the 3nd position on my rack. Now theres $n-3=15-3=12$ marbles left.
\item I grab 1 marble, its clear. Ok, I'll put it on the 4nd position on my rack. Now theres $n-4=15-4=11$ marbles left.
\end{itemize}
Thats what sampling with replacement looks like. \\
--- Overall: I have a bag of 15 marbles, and my sample was ``I chose 4 marbles". 

In this scenario, keep the marbles in a rack, in order. (each time, theres 1 less marble, noting that i can only do this a max of $n=15$ times).\\
--- So this time I got $\omega =$(red, blue, red, clear).\\

\noindent \textbf{How many outcomes are there? Logically.} \\
Well, simply put:\\
The first marble has $n$ posibilities. \\
The second marble has $n-1$ posibilities.\\
The third marble has $n-2$ posibilities.\\
The...$~~~~~~~~~~~~~~~~~~~~~~~~$...posibilities.\\
$\downarrow ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\downarrow$\\
The $k$th marble nas $n-k$ posibilities, noting that $k\ngtr n$ since you can't pick up more than n marbles. \\
This does imply that the $k$th marble has only 1 posibility if instead $k\geq n$, so we should say it has $(n-k) + 1$ posibility.

So we have $$n*(n-1)*(n-2)* \cdots *(n-k+1) = nPk.$$
Where we say $nPk$ is the number of $k$ permutations of $n$.

The number of outcomes is easier to calculate if we rewrite this formula as...
$$\#\Omega = nPk = \frac{n!}{(n-k)!}$$
since 
$$n*(n-1)*(n-2)* \cdots *(n-k+1)$$
$$ = \frac{n!}{(n-k)(n-k-1)(n-k-2)\cdots 3*2*1}.~~~\square$$

\noindent \textbf{How many outcomes are there? Proof wise.}\\
\textit{Proof.}
Let $S=\{1,2,...,n\}~for~some~n\in \mathbb{N}$.\\
Set an element of our sample space $\omega \in \Omega$ to be an ordered $k$-tuple of elements $s_i \in S$.\\
So $\omega = (s_1, s_2, s_3,..., s_k), ~for~some~k\in \mathbb{N}$.\\
But this time this tuple has elements $s_i\in S$,  that are distinct. So, $s_i \neq s_j, ~\forall i\neq j.$\\
Then \\
$$\Omega = \big\{ (s_1, s_2, ..., s_k)_{k\in \mathbb{N}} ~\big|~ s_i \in S, s_i \neq s_j, ~\forall i\neq j. \big\}$$
but this implies
$$n*(n-1)*(n-2)* \cdots *(n-k+1) = nPk$$
$$\textstyle =n*(n-1)*(n-2)* \cdots *(n-k+1)*\frac{(n-k)(n-k-1)(n-k-2) *\cdots *3*2*1}{(n-k)(n-k-1)(n-k-2) *\cdots *3*2*1}$$
$$\textstyle =\frac{n*(n-1)*(n-2)* \cdots *(n-k+1)(n-k)(n-k-1)(n-k-2) *\cdots *3*2*1}{(n-k)(n-k-1)(n-k-2) *\cdots *3*2*1}$$
$$=\frac{n!}{(n-k)!}.$$
So therefore
$$ \#\Omega = nPk = \frac{n!}{(n-k)!}.$$\hfill$q.e.d.$
\newpage



\subsection{Sampling without replacement, \\ order doesn't matter}
Suppose we have a sample space with $n$ elemtents. \\
--- I have a bag of $n=15$ marbles. They are colored.\\
Now suppose we want to take a sample of size $k$. \\
--- I want to pick $k=4$ marbles.\\

\noindent Let's choose: \\
We don't care about order, so we will just hold it in our hand.
\begin{itemize}
\item I grab 1 marble, its red. It is in my hand.
\end{itemize}
We are not sampling with replacement, so after we pick one I pick up another one and keep it in my hand.
\begin{itemize}
\item I grab 1 marble, its blue. Ok, I'll keep this in my hand.
\end{itemize}
Then we repeat until we get our full sample (of size $k$). 
\begin{itemize}
\item I grab 1 marble, its red. Ok, I keep this in my hand.
\item I grab 1 marble, its clear. Ok, I keep this in my hand.
\end{itemize}
Thats what sampling with replacement looks like. \\
--- Overall: I have a bag of 15 marbles, and my sample was ``I chose 4 marbles". 

In this scenario, Imma put back the marbles each time (so I could get the same one each time). Also in this scenario, I care about the order. \\
--- So this time I got the \textbf{set} $\omega =\big\{2~red's, blue, clear\big\}$ in my hand.\\

\noindent \textbf{How many outcomes are there? Logically.} \\
Well, simply put:\\
The first marble has $n$ posibilities. \\
The second marble has $n-1$ posibilities.\\
The third marble has $n-2$ posibilities.\\
The...$~~~~~~~~~~~~~~~~~~~~~~~~$...posibilities.\\
$\downarrow ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\downarrow$\\
The $k$th marble nas $n-k$ posibilities, noting that $k\ngtr n$ since you can't pick up more than n marbles. \\
This does imply that the $k$th marble has only 1 posibility if instead $k\geq n$, so we should say it has $(n-k) + 1$ posibility.

So we have $$n*(n-1)*(n-2)* \cdots *(n-k+1).$$

The number of outcomes is easier to calculate if we rewrite this formula as...
$$\#\Omega = nPk = \frac{n!}{(n-k)!}$$
since 
$$n*(n-1)*(n-2)* \cdots *(n-k+1)$$
$$ = \frac{n!}{(n-k)(n-k-1)(n-k-2)\cdots 3*2*1}.$$

Now lastly note that for each event $\omega$, it is a proper subset of S. Also this is not an ordered $k$-tuple. So each $\omega$ can be rearranged in many ways. I.E. The permutations do not matter, what we care about is the \textbf{combinations}.

We can kind of cheat and make this simpler, since we are only picking up $k$ amount, we don't have to consider all of the possible unordered subsets, just the $k$ amount of subsets we pick. Each subset can be rearranged in $k!$ ways. So out ot the $n!$ posibilities, the $(n-k)!k! \implies (n-k)!k!$.

\noindent And so, 
$$\#\Omega = \frac{n!}{(n-k)!k!} = nCk ={n \choose k},$$
which makes sense since $nPa \geq nCk.~~~\square$\\
--- Read: ``$n$ choose $k$"

\noindent \textbf{How many outcomes are there? Proof wise.}\\
\textit{Proof.}
Let $S=\{1,2,...,n\}~for~some~n\in \mathbb{N}$.\\
Set an element of our sample space $\omega \in \Omega$ to be an unordered set with elements $s_i\in S$. Since we are going without replacement, the amount of elements in each subset have to add up to the total amount of elements in our sample space, i.e. $\sum (\#s_i \in  \omega _i) = \#\Omega$. But this also imples that all $\omega_i$ are proper subsets of S. \\
So $$\Omega = \big\{  \omega \subset S ~\big|~ \# = k,~1\leq k \leq n  \big\}.$$
Now we know for the amount of permutations of $\Omega$ is,
$$ \#\Omega = nPk = \frac{n!}{(n-k)!}.$$
And the total amount of subsets $\omega$, has been counted $k!$ times. (first $k$ has been counted $k$ times = $k$ permutations). \\
So we need to delete this amount from our number of permutations formula\\
but this implies\\
$${nCk} ~\implies~{ \frac{nPk}{k!} = \frac{n*(n-1)*(n-2)* \cdots *(n-k+1)}{k!}}$$
which we know we can simplify to
$$nCk = \frac{n!}{k!(n-k)!}.$$\hfill$q.e.d.$





\subsection{Sampling with replacement,\\ order doesn't matter}
Suppose we have a sample space with $n$ elemtents. \\
--- I have a bag of $n=15$ marbles. They are colored.\\
Now suppose we want to take a sample of size $k$. \\
--- I want to pick $k=4$ marbles.\\

\noindent Let's choose: \\
We don't care about order, so we will just mark it down as a set/combination.
\begin{itemize}
\item I grab 1 marble, its red. Marked down 1 red.
\end{itemize}
We want the to sample with replacement, so after we pick one, we put it back in the bag. 
\begin{itemize}
\item I got one blue, I'll put this back in the bag (so theres a chance I could pick it again).
\end{itemize}
Then we repeat until we get our full sample (of size $k$). 
\begin{itemize}
\item I grab 1 marble, its red. Ok, I'll put it back. 
\item I grab 1 marble, its clear. Ok, I'll put it back.
\end{itemize}
That's what sampling with replacement looks like. \\
--- Overall: I have a bag of 15 marbles, and my sample was ``I chose 4 marbles". 

In this scenario, Imma put back the marbles each time (so I could get the same one each time). Also in this scenario, I care dont care about the order, just the combination.\\
--- So this time I got the \textbf{set} $\omega =\big\{2~red's, blue, clear\big\}$.

\noindent \textbf{How many outcomes are there? Logically.} \\
Well, simply put:\\
We want combinations of these, so we will use 
$$\Omega = {n \choose k} = \frac{n!}{k!(n-k)!}.$$
But this time, we can take any combination of each combination. So consider each combination's elements, $x_i \in \omega,$ (where $\omega= (x_1,x_2,x_3...x_i)$). 

Then what I am saying is that we can rearrange $x_i$ in many different ways. For simplicity sake (and by our professor's idea), consider using ``dividers". In this fasion, there are $k$ distinct samples from the $nCk$ equation. And for each sample we can then take each $x_i$ and split it into its components. \\
E.g. $x_1 = 2~red = 1~red+1~red = 0+2~red = 2~red+0,$ where in this case, we treat + as the divider.

Similarly, we can have a box of donuts, and there are ``$\infty$-many" choices (w/ replacement i.e. the same flavors), as well as the order doesn't matter, ``we got 3 chocolate, 4 strawberry, and 5 glazed". Then we can ``break down" each \\
$x_i = \{ 3~chocolate,~4~strawberry,~5~glazed \}$ into  \\
$x_i = \{ 1~chocolate,~ 1~chocolate,~ 1~chocolate,~1~strawberry,~ 1~strawberry,~\cdots ~1~glazed,~1~glazed,\}$\\
and then use dividers to split this into as many partitions as we would like.

Now how many dividers are there again?\\
Well each solution can be represented by $k$ elements, $x_1,...,x_k$.\\
And then there should be $n-1$ lines since there are only $n$ options (colors, flavors of donuts, etc.)

And the main idea is we can ``permute" these dividers whereever we like. To cheat though, we can just consider \textbf{all} the combinations of the sample points (colors/flavors) + all the dividers, compared to how many we pick ($k$ amount (which can be larger than the $\#\Omega$).

So finally our equation is 
$$ {n \choose k} \implies {[n + (k-1)] \choose k} = {n+k-1 \choose n-1}.~~~\square$$

\noindent \textbf{How many outcomes are there? Proof wise.}\\
\textit{Proof.}
Let $S=\{1,2,...,n\}~for~some~n\in \mathbb{N}$.\\
Set an element of our sample space $\omega \in \Omega$ to be an unordered set with elements $s_i\in S$. Since we are going without replacement, the amount of elements in each subset have to add up to the total amount of elements in our sample space, i.e. $\sum (\#s_i \in  \omega _i) = \#\Omega$. But this also imples that all $\omega_i$ are proper subsets of S. \\
So $$\Omega = \big\{  \omega \subset S ~\big|~ \# = k,~1\leq k \leq n  \big\}.$$
Now we know for the amount of permutations of $\Omega$ is,
$$ \#\Omega = nPk = \frac{n!}{(n-k)!}.$$
And the total amount of subsets $\omega$, has been counted $k!$ times. (first $k$ has been counted $k$ times = $k$ permutations). \\
So we need to delete this amount from our number of permutations formula\\
but this implies\\
$$nCk ~\implies{}~ \frac{nPa}{k!} = \frac{n*(n-1)*(n-2)* \cdots *(n-k+1)}{k!}$$
which we know we can simplify to
$$nCk = \frac{n!}{k!(n-k)!}.$$
Now, consider the elements of our sample space $\omega_i \in \Omega$.\\
Note that since $\omega\in \Omega$, then for $\omega \ni n_i \in S$.\\
But now we can partition $\omega$ and its $k$ elements $n_1,...,n_i,...n_k$, with $n-1$ disjoint unions.\\
This implies there are now $k + (n-1)$ "objects", which we can take any combination of. Where for each instance of two adjacent disjoint unions, we take the union of the empty set, i.e. $\cdots \dot{\cup}~\dot{\cup} \cdots = \cdots \dot{\cup}~\emptyset~\dot{\cup} \cdots.$\\
So then we have \\
$${n \choose k} \implies {[n + (k-1)] \choose k} \implies {n + k-1 \choose k}= {n+k-1 \choose n-1}. ~~~~~\square$$

Summary: Just Memorize
\begin{enumerate}
\item Sample with replacement, order matters := $n^k$
\item Sampling without replacement, order matters := \\$nPk = \frac{n!}{(n-k)!}$
\item Sample without replacement, order doesn't matter := $nCk = {n \choose k} = \frac{n!}{k!(n-k)!}$
\item Sample with replacement, order doesn't matter := \\${k+n-1 \choose n-1}={k+n-1 \choose k}$ 
\end{enumerate}
As well as, for grabbing marbles one by one...
\begin{enumerate}
\item Sampling with replacement $\implies{}$ putting marbles back into the bag
\item Sampling without replacement $\implies{}$ keeping the marbles out of the bag
\item Order matters $\implies{}$ put it in on a rack in order, or mark down the tuple (which came first) $\implies{}$ permutations
\item Order doesn't matter $\implies{}$ hold it in your hand, or mark down the set $\implies{}$ combinations
\end{enumerate}

\subsection{Random Sampling Examples}

\textbf{E.g. 1.8} Suppose a bag of scrabble tiles contains $5~E's,4~A's,$ \\$3~N's,$ and $2~B's$. Draw $4$ tiles without replacement.
\begin{itemize} \item[(a)] Find $P(C)$ by imagining that the tiles are drawn one by one, as an ordered sample.\end{itemize}
\textit{Solution.}
Let $C = \{ 2~E,~1~A,~1~N \}$. (Note sets are unordered)
Suppose you have
\begin{align*}
&P(1st~E,~2nd~E,~3rd~A,~4th~N)\\
&+ P(1st~E,~2nd~A,~3rd~E,~4th~N)\\
&+ P(1st~N,~2nd~A,~3rd~E,~4th~E) + \cdots
\end{align*}
$$ = \frac{(5*4)*(4)*(3)}{14*13*12*11} + \frac{(5*4)*(4)*(3)}{14*13*12*11} + \frac{(3)*(4)*(5)*(4)}{14*13*12*11}+\cdots$$
$$=\frac{5*4*4*3}{14*13*12*11} {4 \choose 2}{2\choose 1}{1\choose 1}$$
$$=\frac{5*4*4*3}{14*13*12*11} \left( \frac{4*3}{2*1} \right) \left( \frac{2*1}{1} \right) \left(\frac{1}{1}\right) \approx 0.0599. ~~~\square$$
\begin{itemize}\item[(b)] Find $P(C)$ by imaging that the tiles are drawn all at once as an unordered sample.\end{itemize}
Or more simply,
$$\frac{ {5\choose 2}{3\choose 1}{4\choose 1}  }{{14\choose 4}}.$$
which gives the same value, $\approx 0.059$.$~~~\square$\\

\noindent \textbf{E.g. 1.8} Everyday a kindergarten class chooses randomly one of the 50 state flags to hang on the wall, without regard to previous choices.
\begin{itemize}\item [(a)] Find the sample space.\end{itemize}
\textit{Solution.} Let $S=\{1,2,3,4,...,50\}$, then 
$$\Omega=S\times S\times S = \big\{(i,j,k) ~\big|~ i,j,k\in S \big\}.$$
and then $$\#\Omega = 50*50*50 = 50^3.~~~\square$$
\begin{itemize}\item [(c)] Find the probability that the Wisconsin flag will be hung at least two of the three days, Monday, Tuesday, and Wednesday.\end{itemize}
\textit{Solution.}\\
Lets seperate this into the probability of 2 days, and probability of 3 days (which includes 2).\\
$$P(W~W~else)+P(W~else~W)+P(else~W~W)+P(W~W~W) $$
$$= \frac{1*1*49}{50*50*50}*3+\frac{1*1*1}{50^3}.~~~\square$$


\noindent \textbf{E.g. 1.13} Suppose we have a class of $24$ students.
\begin{itemize}\item [(a)] A team of three children is chosen at random. Find the probability that the team consists of Shane, Heather, and Laura.\end{itemize}
\textit{Solution.} \\
So we need to find 
$$P\big(\{Shane, ~Heather, ~Laura\}\big) = \frac{1}{{24 \choose 3}}.~~~\square$$
\begin{itemize}\item [(c)] Find the probability that Mary is on the team.\end{itemize}
\textit{Solution.} 
$$P\Big(\big\{ \{Shane, ~Heather, ~Laura\} \land \{Mary\} \big\}\Big) = \frac{{23\choose 2}}{{24\choose 3}}.~~~\square$$

\noindent \textbf{E.g. ?.?} Three students are chosen to be class president, vice president, and treasurer. No student can hold more than one office. \\
\begin{itemize}\item [(a)] Find the probability that mary is president, cory is vice president, and matt is treasurer.\end{itemize}
\textit{Solution.}\\
Let event $A$ be the event in part (a) holds. Then 
$$P(A) = \frac{1*1*1}{24*23*22}.~~~\square$$
\begin{itemize}\item [(b)] What is the probability that ben is either president or vice president.\end{itemize}
\textit{Solution.}\\
Let event $B$ be the event in part (b) holds. Then 
$$P(A) = \frac{1*23*22}{24*23*22} + \frac{23*1*22}{24*23*22} = \frac{1}{24}+\frac{1}{24} = \frac{1}{12}.~~~\square$$

%%%------------------------ Lecture 4: 1/25/23
\noindent \textbf{E.g. 1.7} We have an urn with 3 green and 4 yellow balls. We draw 3 one by one without replacement.
\begin{itemize}\item [(a)] Find the probability that the colors we see in order are green, yellow, green.\end{itemize}
\textit{Solution.} \\
Note: order matters, without replacement. So,
$$P(\omega) = \frac{3*4*5}{7*6*5}~.~~~\square$$
\begin{itemize}\item [(b)] Find the probability that our sample of 3 balls contain 2 green balls and 1 yellow ball.\end{itemize}
\textit{Solution.} \\
Note: order doesn't matter, without replacement. So,
$$P(\omega) = P (y,g,g) + P (g,y,g) + P(g,g,y)$$
$$=\frac{3*4*2}{7*6*5} + \frac{3*2*4}{7*6*5} + \frac{4*3*2}{7*6*5}$$
$$=\frac{3*4*2}{7*6*5} * {3 \choose 1} = \frac{3*4*2}{7*6*5} * 3 = \dfrac{{3\choose 2}{4\choose 1}}{{7\choose3}}~.~~~\square$$

\noindent \textbf{E.g. 1.10} We roll a fair die repeatedly until we see the number four appear and then stop. The outcome of the expirement is the number of rolls.
\begin{itemize}\item [(a)] Find the sample space $\Omega$ and the probability measure $P$. \end{itemize}
\textit{Solution.} \\
The sample space goes on until infinity with discrete steps, so
$$ \Omega = \big\{ 1,2,3,4,... \big\} \cup \big\{ \infty \big\}.$$
Also our probability measure should be for a finite amount of trials, set this amount to be ``$k$-trials". Then\footnote{Technnically, we should define a probability measure using the definition. So for and event that is also $A\subseteq \Omega$, we set $P(A)=\sum\limits_{k\in A} P(A)$.},
$$ P(k) = \frac{5^{k-1}*1}{6^k}.~~~\square$$
\begin{itemize}\item [(b)] Calculate the probability that the number never appears.\end{itemize}
\textit{Solution.} \\
The probability that the number never appears implies the probability that there are infinite events.
Which is
$$P(\infty)=1-\sum\limits_{k=1}^{\infty} P(k) = 1- \sum\limits_{k=1}^{\infty} \frac{5^{k-1}}{6^k}$$
$$ = 1-\frac{a}{1-r} = 1-\frac{\frac{1}{6}}{1-\frac{5}{6}}$$
$$ = 1-\frac{\frac{1}{6}}{\frac{1}{6}} = 1-1 = 0.$$
So,\footnote{This such definition can be taken in a limit process as well, at least in this case. So you would set $\{\infty \} \subseteq \{F,F,F...,F\},$ of size $k$. Or better defined, $|\{F,F,F...,F\}|=k,~\forall k\in \mathbb{N}$. Then $P(\infty) \leq P(F...F) = \frac{5^k}{6^k} = \left( \frac{5}{6} \right)^k \rightarrow 0,$ as $k\rightarrow 0$. This is opposite to the probability I get a 100\% in this class, so at least I have that going for me.}
$$P(\infty) = 0.~~~\square$$






\section{Infinite Many Outcomes}
\hypertarget{infinitely many outcomes}{Key Idea:} infinite/uncountable sets $\implies{}$ probabilities are continuous over $\mathbb{R}$. ($\mathbb{R}$ is the new thing here). We shall look at some examples, it should be fairly simple.\\

\subsection{Discrete Infinite Outcomes}
\noindent \textbf{E.g. 1.16} Flip a coin until the first tails comes up. Record the number of flips required as the outcome of the experiment.\\
\begin{itemize} \item[(a)] Find the sample space.\end{itemize}
\textit{Solution.}
Then the sample space is  $$\Omega = \{1,2,3,...\}\cup \{\infty\}.~~~\square$$
\begin{itemize} \item[(b)] Find $P(k)$.\end{itemize}
\textit{Solution.}
Well, there are $n=2$ posibilities for each sample point.\\
The $k$th event can be described as the following,\\
$k=5 ~\implies~H*H*H*H*H*T.$\\
So $$P(k) = \frac{1*1*1*\cdots *1}{2^k}, ~for~k=1,2,3,...$$
And also 
$$P(\infty) = 1- \sum\limits_{k=1}^{\infty} P(k)= 1- \sum\limits_{k=1}^{\infty} \frac{1}{2^k},$$
which is a geometric series, with $a_1 = \frac{1}{2}$, common ratio $r=\frac{1}{2}$.\\
So since this one starts at $k=1$, 
$$1- \sum\limits_{k=1}^{\infty} \frac{1}{2^k} = 1-\frac{a}{1-r} = 1-\frac{\frac{1}{2}}{1-\frac{1}{2}}$$
$$=1-\frac{\frac{1}{2}}{\frac{1}{2}}=1-1 = 0.$$
So the probability that you never get tails/you keep getting heads, in this limit process of a \textbf{fair} coin, is 
$$P(\infty) = 0.~~~\square$$

\noindent \textit{Remark.} Discrete Infinite Outcomes $\implies{}$ an Infinite Series.

\subsection{Continuous Infinite Outcomes}
\noindent \textbf{E.g. 1.17}- We pick a real number uniformly at random reom the closed interval $[0,1]$.\\
\begin{itemize} \item[(a)] Find the sample space. \end{itemize}
\textit{Solution.}
$$\Omega = \{a\in \mathbb{R} ~|~ 0\leq x \leq 1\} = [0,1].~~~\square$$
\begin{itemize} \item[(b)] Find $P\left(\frac{1}{2} \leq x \leq \frac{3}{4}\right)$\end{itemize}
\textit{Solution.}
$$P\left(\frac{1}{2} \leq x \leq \frac{3}{4}\right) = \frac{\frac{3}{4} - \frac{1}{2}}{1} = \frac{1}{4}.~~~\square$$

\noindent \textbf{E.g. 1.18} Consider a dartboard in the shape of a disk with radius of 9 inches. The bullseye is a disk of diameter $\frac{1}{2}$ inch in the middle of the board. What is the probability that a dart randomly thrown on the board hits the bullseye?\\
\textit{Solution.}
$$P(hitting~the~bullseye) = \frac{Area(bullseye)}{Total~area}$$
$$ = \frac{\pi \left( \frac{1}{9} \right)^2}{\pi (9)^2} = \frac{\frac{1}{16}}{81} = \frac{1}{(16)(81)}.~~~\square$$

\noindent \textit{Remark.} Continuous Infinite Outcomes $\implies{}$ an Area/Length (Depending on the Sample Space, usually Euclidean space $\mathbb{R}^n$, also usually $n=1,2$).




%%%--------------------- Lecture 5: 1/30/23
\section{Consequences of \\the Rules of Probability} 
These are the three main properties that come from the Rules of Probability (i.e. the way we had described and defined our $\sigma$-algebra ``$P(A),A\in\mathcal{F}=|\Omega|$" and Probability Space)
\begin{enumerate}
\item Decomposition of an event.
\begin{itemize} \item If $A = \bigcup\limits_{i=1}^\infty A_i$, where $A_1,A_2,...$ are pairwise disjoint events, then $P(A) = \sum\limits_{i=1}^\infty P(A_i)$.\end{itemize}
\item Events and Complements
	\begin{itemize} 
		\item $P(A)+P(A^C)=1$
		\item $P(A^C)=1-P(A)$
	\end{itemize}
\item Monotonicity of Probability
\begin{itemize} \item If $A\subseteq B,$ then $P(A)\subseteq P(B)$.\end{itemize}
\end{enumerate}
Below we will give examples and prove some of these consequences.

\subsection{Decomposing an event}
If $A = \bigcup\limits_{i=1}^\infty A_i$, where $A_1,A_2,...$ are pairwise disjoint events, then $P(A) = \sum\limits_{i=1}^\infty P(A_i)$

\noindent \textbf{E.g. 1.19} Suppose an urn contains 30 red, 20 green and 10 yellow balls. Draw two without replacement. What is the probability that the sample contains exactly one red or exactly one yellow?\\ 
\textit{Solution}\\
So the question is asking, $P(exactly~1~red~or~exactly~1~yellow)$.\\
\textbf{Main question:} Can you decompose this into other probabilities?\\
Without replacement $\implies{}$  means all the events can be broken up into disjoint unions. So therefore its probabilities can also be broken up.\footnote{You have to be very careful when you can or can't do this. }\\
\textbf{Ans:} Yes you can! i.e.\\
$P(exactly~1~red~or~exactly~1~yellow)$\\

$= P\big(   ~\{1r,1y\} \cup \{1r,1g\} \cup \{1y,1g\}~   \big)$\\

$=P\big(  \{1r,1y\}  \big) + P\big(  \{1r,1g\}  \big) + P\big(  \{1y,1g\}  \big)$\\

$=\dfrac{{30\choose1}{10\choose1}}{{60\choose2}} + \dfrac{{10\choose1}{20\choose1}}{{60\choose2}} + \dfrac{{30\choose1}{20\choose1}}{{60\choose2}}$\\

$=\dfrac{{30\choose1}{10\choose1}+{10\choose1}{20\choose1}+{30\choose1}{20\choose1}}{{60\choose2}}$\\

$=\dfrac{30*10+10*20+30*20}{\frac{60!}{2!*58!}}$\\

$= \dfrac{300+200+600}{    \frac   {\cancelto{30}{60} *59*\cancel{58!}}  {\cancelto{1}{2} *\cancel{58!}    }      }$ \\

$= \dfrac{1100}{30*59} = \dfrac{550}{15*59} = \dfrac{110}{3*59}=\dfrac{110}{117}.~~~\square$\\

\noindent \textbf{Recall} If $A_{1},A_{2}, ...$ are pairwise disjoint events, then $$P\left(\bigcup\limits_{i=1}^{\infty}A_{i}\right) = \sum\limits_{i=1}^{\infty}P(A_{i}).$$


\noindent \textbf{E.g. 1.20} Peter and Mary take turns rolling a fair die. If Peter rolls 1 or 2 he wins and the game stops. If Mary rolls 3, 4, 5, or 6, she wins and the game stops. They keep rolling in turn until one of them wins. Suppose Peter rolls first.
\begin{itemize}\item [(a)] What is the probability that Peter wins and rolls at most 4 times?\end{itemize}
\newpage \textit{Solution.} 
Let $A_i=\{Peter~wins~at~his~i^{th}~roll\}$\\
Say the game looks like this...\\
I order it as ``step number, the player, if they win or not, and probability that they would have won"\\
$$\begin{aligned}
1.~Peter : F : \frac{4}{6}\\
2.~Mary : F : \frac{2}{6}\\
3.~Peter : F : \frac{4}{6}\\
4.~Mary : F : \frac{2}{6}\\
.\\
.\\
.\\
(i).~Peter : T : \frac{4}{6}\\
\end{aligned} $$
Then 
\begin{align*}
&P(Peter~wins~and~rolls~at~most~4~times)\\\\
&=P(A_{1}) + P(A_{2}) + P(A_{3}) + P(A_{4}) \\\\
\end{align*}
\begin{align*}
\\&=\displaystyle\sum\limits_{i=1}^{4} \left(\frac{4}{6}* \frac{2}{6}\right)^{i-1} \left(\frac{2}{6}\right)\\\\
&=\frac{1}{3}\sum\limits_{i=1}^{4} \left(\frac{2}{9}\right)^{i-1}\\\\
&=\frac{1}{3} \left[ \left(\frac{2}{9}\right)^{0} + \left(\frac{2}{9}\right)^{1}+ \left(\frac{2}{9}\right)^{2} + \left(\frac{2}{9}\right)^{3} \right] \\\\
&=\frac{1}{3} \left[ \frac{1}{3} + \frac{2}{9} + \left(\frac{2}{9}\right)^{2} + \left(\frac{2}{9}\right)^{3} \right] \\\\
&=\frac{1}{3} ~~\dfrac{1-\left(\dfrac{2}{9}\right)^{4}}{1-\left(\dfrac{2}{9}\right)} \approx 0.427~~~\square
\end{align*}
\begin{itemize}\item [(b)] What is the probability that May wins?\end{itemize}
\textit{Solution.}
Let $B_i=\{Mary~wins~at~her~i^{th}~roll\}$\\
Say the game looks like this...\\
$$\begin{aligned}
1.~Peter : F : \frac{4}{6}\\
2.~Mary : F : \frac{2}{6}\\
\end{aligned} $$
$$\begin{aligned}
3.~Peter : F : \frac{4}{6}\\
4.~Mary : F : \frac{2}{6}\\
5.~Peter : F : \frac{4}{6}\\
6.~Mary : F : \frac{2}{6}\\
.\\
.\\
.\\
\end{aligned} $$
Then 
$$
\begin{aligned}
P(Mary~Wins)&=\sum\limits_{i=1}^{\infty} P(B_{i})\\\\
&= \sum\limits_{i=1}^{\infty} \left[ \left(\frac{4}{6}\right) \left(\frac{2}{6}\right) \right]^{i-1} \left(\frac{4}{6}\right)\\\\
&=\sum\limits_{i=1}^{\infty} \left(\frac{2}{9}\right)^{i-1} \left(\frac{2}{3}\right)\\\\
&= \left(\frac{2}{3}\right)^{2} \frac{a}{1-r}\\\\
&=\left(\frac{2}{3}\right)^{2} \frac{1}{1-\frac{2}{9}}\\
&=\left(\frac{2}{3}\right)^{2} \left(\frac{9}{7}\right) = \frac{4}{7} \approx 0.571~~~\square
\end{aligned}
$$




\newpage
\subsection{Events and Complements}
Both shall hold, 
\begin{itemize}
\item $P(A)+P(A^C)=1$
\item $P(A^C)=1-P(A)$
\end{itemize}

\noindent \textbf{E.g. 1.21} Roll a fair die 4 times. What is the probability that some numbers appears more than once?\\
\textit{Solution.}
$$
\begin{aligned}
&P(some~\#'s~appear~more~than~once)\\\\
&=1-P(all~numbers~are~different)\\\\
&=1-\frac{\cancel{6}*5*\cancelto{\cancel{2}}{4}*\cancel{3}}{\cancel{6}*6*\cancelto{3}{6}*\cancelto{\cancel{2}}{6}}\\\\
&=1-\frac{5}{18} = \frac{13}{18}=.722~~~\square
\end{aligned}
$$




\subsection{Monotonicity of Probability}
\textbf{Claim:} If $A\subseteq B,$ then $P(A)\subseteq P(B)$.\\
\textit{Proof.} 
Let $A,B$ be sets in $\Omega$.
Suppose $A\subset B$.\\
Then $B=A~\dot{\cup}~(B\backslash A)$\\
($\dot{\cup}$ meaning disjoint union).\\
Since this is a disjoint union, we can take the individual probabilities.\\
So,\\
$P(B)=P(A)+P(B\backslash A) \geq P(A) +0 =P(A)$\hfill $q.e.d.$\\

\noindent \textbf{E.g. 1.22} Suppose we toss a fair coin repeatedly. What is the probability that tails never occurs?\\
\textit{Recall.} We know $P(tails~never~occur)=0$ \hyperlink{infinitely many outcomes}{already}. This is by using a a summation generalized to $k$, and as it approaches $\infty$, we find $P(A)\to 0$.\\
However lets prove this using the new property ``Monotonicity of Probability" and the squeeze theorem.\\

\noindent \textit{Solution.} 
We know
\begin{align*}
0&\leq P(tails~never~occur) \\
&\leq P(tails~never~occur~in~the~1st~k~tosses)\\
&=\left(\frac{1}{2}\right)^{k},~\forall k\in \mathbb{N}.\\
\end{align*}
Then by the right hand side inequality, you may see, how the squeeze theorem will be used. Also the ratio $\left(\dfrac{1}{2}\right)^{k}$ is because each $k$th flip has a probability measure of $\frac{1}{2}$.\\
So now 
$$\lim\limits_{k\to \infty} \left(\frac{1}{2}\right)^{k}=0$$
or alternatively 
$$P(tails~never~occur~in~the~1st~k~tosses)=\left(\frac{1}{2}\right)^{k} ~\xrightarrow{k\to \infty}~0.$$
And by the squeeze theorem (from Calculus)
$$P(tails~never~occur)=0.$$\hfill$q.e.d.$

\newpage
\noindent \textbf{Question:} What if the probability measure was not fair?\\
Say a coin wasn't fair and had probability measures\\
$P(\omega) \neq P(H)=P(T)$, and we set\\
$P(H)=0.99 \implies P(T) = 0.01$.\\
But then, like the previous example, we get the same results, since by squeeze theorem
$$\begin{aligned}
0\leq &P(tails~never~occur) \\\leq 
&P(tails~never~occur~in~the~1st~k~tosses)\\
&=\left(0.99\right)^{k} ~\xrightarrow{k\to \infty}~ 0 
,~\forall k\in \mathbb{N}.
\end{aligned}$$



%%%---------------------------- Lecture 6: 2/1/23
\subsection{More Examples of these Consequences}
\noindent \textbf{E.g. 1.15} - An urn contains 4 balls: 1 white, 1 green, and 2 red. We draw 3 balls \textit{with} replacement. Find the probability that we \textit{did not} see all three colors.
\begin{itemize}\item[(a)] Solve with Inclusion-Exclusion Principle.\end{itemize}
\textit{Solution.}\\
Let $W = \{white~ball~did~not~appear\}$, \\
$G = \{green~ball~did~not~appear\}$, and \\
$R = \{red~ball~did~not~appear\}$.\\
Then 
\begin{align*}
&P\{we~did~see~all~three~colors\} \\\\
&= P(W \cup G \cup R) \\\\
&=P(W) + P(G) + P(R)\\
&P(W \cap G) -P(W \cap R) -P(G \cap R)\\
&+P(W\cap G\cap R)\\\\
\end{align*}

\begin{align*}
&=\frac{3*3*3}{4*4*4} + \frac{3*3*3}{4*4*4} + \frac{2*2*2}{4*4*4}\\
&-\frac{2*2*2}{4*4*4} - \frac{1*1*1}{4*4*4} - \frac{1*1*1}{4*4*4}\\
&+\frac{0*0*0}{4*4*4}\\\\
&=\frac{3^{3}+3^{3}-1-1}{4^{3}}=\frac{52}{64} = \frac{26}{32} = \frac{13}{16}.~~~\square
\end{align*}
\begin{itemize}\item[(b)] Solve using the complement.\end{itemize}
\textit{Solution.}\\
\begin{align*}
&P\{we~did~see~all~three~colors\}\\\\
&=1-P(we~saw~all~three~colors)\\\\
&=1- \frac{1*1*2*3!}{4*4*4}\\\\
&=1- \frac{12}{64} = 1- \frac{3}{16} =\frac{13}{16}.~~~\square
\end{align*}




\subsection{Inclusion-Excluion Principle}
\textbf{The goal is to rewrite unions as intersections.} ``This is because unions overlap and are difficult to calculate, but individual sets and intersections are simpler to calculate."\\
\newpage
\noindent \textbf{For 2 sets:}
$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$
The probability that one of the sets occur, implies probability of A and B, but then we are counting the intersection twice. This is why we subtract one of the intersections.\\

\noindent \textbf{For 3 sets:}
Similarly,
\begin{align*}
P(A\cup B \cup C)=&P(A)+P(B)+P(C)\\
&-P(A\cap B) - P(A\cap C) - P(B\cap C) \\
&+ P(A\cap B\cap C)\\
\end{align*}

We add the last triple intersection, because when we delete the pairwise set intersections, we delete all events from the middle. One may draw out these examples, or I may find pictures to reference.\\

\noindent \textbf{For $n$ sets:}
Due to the parity (odd or even amount) of the number of sets, 
\begin{itemize}
\item When its even, we have overlapping intersections we need to delete
\item When its odd, we delete the even tuples, but then we completely cancel out the odd tuples. i.e. we add back the intersection
\end{itemize}
So we can think of this as (1)\footnote{I would memorize (1), and know what it means well. Then memorize (3) for other computation.}
\begin{align*}
&|A \cup B \cup C \cup D\cup \cdots| = \\
& +|A|+|B|+|C|+|D|+\cdots (add~all~singletons)\\
& -|A\cap B|-|A\cap C|-|A\cap D|-\cdots (minus~all~pairs)\\
& +|A\cap B \cap C |+|A\cap B \cap D|+\cdots (plus~all~triples)\\
& -quadruples\\
& +quintuples\\
& \pm etc.
\end{align*}

And we know we can take probabilities of these sets, so in general it would look like (2)
\begin{align*}
&P(A_{1}\cup A_{2} \cup A_{3} \cup \cdots \cup A_n)\\
&= P(A_{1}) + P(A_{1}) + \cdots +P(A_{n}) \\
& -\sum\limits_{i\leq i<j\leq n} P(A_{i}\cap A_j)\\
& + \sum\limits_{i\leq i<j\leq n} P(A_{j}\cap A_{j} \cap A_{k}) \\
&-~~~\cdots\\
&+ \sum\limits_{1\leq i_{1}<i_{2}<\cdots<i_{k}\leq n} (-1)^{k-1} P(A_{i_{1}}\cap A_{i_{2}} \cap \cdots \cap A_{i_{k}}) \\
& \mp~~~\cdots\\
& +(-1)^{n-1} P(A_{1} \cap \cdots \cap A_{n}).
\end{align*}


Or more concisely (3)\small
$$\boxed {P(A_{1} \cup \cdots \cup A_{n})=\sum\limits_{k=1}^{n} (-1)^{k-1} \sum\limits_{1\leq i_{1} <i_{2} < \cdots < i_{k} \leq n} P(A_{i_{1}} \cap \cdots \cap A_{i_{k}}) }$$
\normalsize

\noindent \textbf{E.g. 1.27} Suppose $n$ people exchange their hats randomly. Find the probability that no one gets his/her own hat.\\

\noindent \textit{Solution.} \\
Let the event $A_{i} = i^{th}~person~gets~his/her~own~hat$. \\
(Note this is the converse of what we want to find.) \\

\noindent Now we can apply what we know about complements and what we just learned about inclusion-exclusion principle. We find 
\begin{align*}
&P(no~one~gets~his/her~own~hat)\\
&=P(A_{1}^{C} \cap A_{2}^{C} \cap \cdots \cap A_{n}^{C})\\
&=1- P\Big((A_{1}^{C} \cap A_{2}^{C} \cap \cdots\cap A_{n}^{C})^{C}\Big)\\
&= 1- P(A_{1}\cup \cdots A_{n}) \\
&= 1-\sum\limits_{k=1}^{n} (-1)^{k-1} \sum\limits_{1\leq i_{1} < i_{2} <\cdots <i_{k} \leq n }  P(A_{i_{1}} \cap \cdots \cap A_{i_{k}})\\
&= 1-\sum\limits_{k=1}^{n} (-1)^{k-1} \sum\limits_{1\leq i_{1} <\cdots <i_{k} \leq n }  \frac{(n-k)!}{n!}\\
&= 1- \sum\limits_{k=1}^{n} (-1)^{k-1} {n\choose k} \frac{(n-k)!}{n!}\\
&= 1-\sum\limits_{k=1}^{n} (-1)^{k-1} \frac{\cancel{n!}}{\cancel{(n-k)!}k!} \frac{\cancel{(n-k)!}}{\cancel{n!}} \\
&= 1+ \sum\limits_{k=1}^{n} (-1)^{k} \frac{1}{k!}\\
&= \sum\limits_{k=0}^{n} \frac{1}{k!} ~\xrightarrow{n\to \infty}~ e^{-1} \approx \frac{1}{2.7}.~~~\square 
\end{align*}
\newpage








%%%------------------------------- Lecture 7: 2/3/23
\subsection{Some Examples}
\noindent \textbf{E.g. 1.14} Assume $P(A)=0.4$ and $P(B)=0.7$. \\Show that $0.1\leq P(AB) \leq 0.4$.\\

\noindent\textit{Proof by Professor:}\\
Notice, $A\cap B \subseteq A \implies P(A\cap B) \leq P(A)=0.4$\\
Also notice, $P(A\cap B)=P(A)+P(B)-P(A\cup B)$, by the inclusion-exclusion principle. \\

\noindent So then
\begin{align*}
1 &\geq P(A\cup B ) = P(A)+ P(B) - P(A\cap B)\\
&= 0.4+0.7-P(A\cap B)=1.1-P(A\cap B)
\end{align*}
Therefore $P(A\cap B) \geq 1.1-1 = 0.1$.\hfill $q.e.d.$\\

\noindent \textbf{E.g. 1.9} We break a stick at a uniformly chosen random location. Find the probability that the shorter piece is less than 1/5 th of the original.\\

\noindent \textit{Solution by Professor:}\\
Let $x\in [0,1],~$ i.e. $x$ is the breaking point of a stick of length/total probability 1.\\
Then notice we can ``break the stick" into two disjoint unions (i.e. 2 sticks). One of the sticks are $x$ with the other $1-x$. Also note, $x\leq \dfrac{1}{2}$ or $x \geq \dfrac{1}{2}$.\\

\noindent So we want $x< \dfrac{1}{5}$, then there are two possible outcomes since $x\leq \dfrac{1}{2}~\lor~x\geq \dfrac{1}{2}$.
\newpage
\begin{align*}
P(x<\frac{1}{2} \cap x\leq \frac{1}{2})+P(1-x < \frac{1}{5} \cap x>\frac{1}{2})\\
= P(x<\frac{1}{5})+ P(\frac{4}{5}<x<1)\\
=\dfrac{1}{5}+(1-\dfrac{4}{5})=\boxed{\frac{2}{5}}~.~~~\square
\end{align*}




\section{Random Variables}
\textbf{Definition. (Random Variable)} Let $\Omega$ be a sample space. A random variable is a function from $\Omega$ into the real numbers.
$$X:\Omega \rightarrow \mathbb{R}.$$

\noindent \textbf{E.g.} Toss a coin. Let $X$ be the number of heads. \\
$\Omega=\{H,T\}$\\
Then, 
\begin{itemize}
\item $X(H)=1$
\item $X(T)=0$
\end{itemize}
$\implies X$ is a random variable.

\subsection{Graphing and Recording \\Table of Random Variables}
\noindent \textbf{E.g. 1.29} Roll a die 2 times. Let $X$ be the sum of two die. Find $P_x(k)=P(x=k),$ for all $k\in X$.\\
\textit{Solution.}\\
Notice $X=\{2,3,4,5,6,7,8,9,10,11,12\}$.

One can count or graph how many events obtain the sum of two die.
i.e. for $x=2$, only one possibility...die \#1=1 and die \#2=1.
Similarly, for 3, one must be 2 and the other 1, so there are two combination.

Alternatively, by graphing (with die \#1 on the ``x-axis", and die \#2 on the ``y-axis")

\begin{figure}[h]
  \includegraphics[width=4in]{PMF of 2 dice.png}
\end{figure}
\noindent With the table of values being
\begin{center}
\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c|c|c| }
\hline
	$x$ & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\
\hline 
	$P_{x}(k)$ & $\frac{1}{36}$ & $\frac{2}{36}$ & $\frac{3}{36}$ & $\frac{4}{36}$ & $\frac{5}{36}$ & $\frac{6}{36}$ & $\frac{5}{36}$ & $\frac{4}{36}$ & $\frac{3}{36}$ & $\frac{2}{36}$ & $\frac{1}{36}$  \\
\hline
\end{tabular}
\end{center}
Note the symmetry for $P_{X}(7)$, also note the non-uniformity, where there is less probabilities for the ``rarer" cases.

We say $P_{X}(k)$ is called the probability mass function of $X$, commonly abbreviated and pronounced ``PMF".\\

\noindent \textbf{Quick E.g.} Using the table. $P(x=2,5,7)=?$\\
\noindent \textit{Solution.}
$P(x=2,5,7)$\\
$= P(x=2)+P(x=5) +P(x=7)$\\
$=P_{x}(2)+P_{x}(5)+P_{x}(7)$\\
$=\dfrac{1}{36} + \dfrac{4}{36} + \dfrac{6}{36}= \dfrac{11}{36}.~~~\square$\\


\subsection{Discrete Random Variable}
\textbf{Definition.} A random variable $X$ is said to be ``discrete" if the set of possible values of $X$ is either finite or countable. \\

\noindent \textbf{E.g. 1.29} Roll a die 2 times. Let $X$ be the sum of two die. Find $P_x(k)=P(x=k),$ for all $k \in X$.\\
This example has discrete random variables since there are finite values of $x$.\\

\subsection{Continuous Random Variable}
Conversely, consider a continuous interval such as $[0,1]\in \mathbb{R}$.\\
We know about the cardinality of $\mathbb{R}$ compared to $\mathbb{N}$, as $\mathbb{R}$ is uncountable.\\
So $x\in [0,1]\implies x$ is continuous, and $x$ has uncountable possible values.\\

\noindent \textbf{E.g. 1.38} We have a dart board of radius 9 inches. The board is divided into four parts by three concentric circles of radii 1, 3, 6 inches. If our dart hits the smallest disk, we get 10 points, if it hits the next region then we get 5 points, and we get 2 and 1 points for the other 2 regions. Let $x$ denote the number of points we get when we throw a dart randomly at the board. Find the probability of $x$.\\

\noindent \textit{Solution.}
Note: we are throwing 1 dart with discrete random variables, since for each zone, we get a single number (that isn't on a continuous range).\\
So we should make a table of values (or plot the points if wanted).\\
Our dart board looks something like this\\
We'll have circles with:
\begin{itemize}
\item $r=1$
\begin{itemize}
\item inside of this, $0<r<1$ is 10 points
\end{itemize}
\item $r=3$
\begin{itemize}
\item $1<r<3$ is 5 points
\end{itemize}
\item $r=6$
\begin{itemize}
\item $3<r<6$ is 2 points
\end{itemize}
\item $r=9$ which is the edge of the board
\begin{itemize}
\item $6<x<9$ is 1 points
\end{itemize}
\end{itemize}

%Visually, 
%
%```desmos-graph
%left=-14; right=14;
%top=10; bottom=-10;
%---
%x^2+y^2=1^2|blue
%(0,1)|blue|label:r=1
%
%x^2+y^2=3^2|blue
%(0,3)|blue|label:r=3
%
%x^2+y^2=6^2|blue
%(0,6)|blue|label:r=6
%
%x^2+y^2=9^2|black
%(0,9)|black|label:r=9
%
%(0,0)|red|label:10 points
%(0,-2)|red|label:5 points
%(0,-4.5)|red|label:2 points
%(0,-7.5)|red|label:1 points
%
%```
%
%
And our table of values I will use to calculate, 
\small
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
	$x$ & 1 & 2 & 5 & 10 \\
\hline
 $P_{x}(k)$ & $\dfrac{\pi (9)^{2} - \pi (6)^{2}}{\pi (9)^{2}}$ & $\dfrac{\pi (6)^{2} - \pi (3)^{2}}{\pi (9)^{2}}$ & $\dfrac{\pi (3)^{2} - \pi (1)^{2}}{\pi (9)^{2}}$ & $\dfrac{\pi (1)^{2}}{\pi (9)^{2}}$   \\
\hline
	$\parallel$ & $\dfrac{45\cancel\pi}{81\cancel\pi}$           & $\dfrac{27\cancel\pi}{81\cancel\pi}$           & $\dfrac{8\cancel\pi}{81\cancel\pi}$            & $\dfrac{\cancel\pi}{81\cancel\pi}$ \\
\hline
	$\parallel$ & $\dfrac{45}{81}$ & $\dfrac{27}{81}$ & $\dfrac{8}{81}$ & $\dfrac{1}{81}$ \\
\hline
	$=$ & $\dfrac{5}{9}$ & $\dfrac{1}{3}$ & $\dfrac{8}{81}$ & $\dfrac{1}{81}$\\
\hline
\end{tabular}
\end{center}
\normalsize
Which are the probabilities of $x$.\footnote{Notice the sum of all probabilities is 1 $\left(\sum P_{x}(k)=1\right)$.}~~~$\square$




\subsection{Probability Mass Function}
We probably should finally define the notion of Probability Mass Functions, as we have already observed in different scenarios. \\

The idea that the sum of all probabilities being 1 is always true for all PMF's. 
This is due to our axiomatic definition of a probability measure, which is a a function $P:\mathcal{F} \to \mathbb{R},~s.t.~P$ has the properties of a $\sigma-$algebra with total mass $1$.\\

That is, 
\textbf{Definition.} Suppose $X=x_{1},x_{2}, ...$\\
Then we say $P_{x}(x_{k})=P(x=x_{k}),$ for $k=1,2,3,...$ is called the probability mass function of $X$.\\
And we know (from paragraph 2) $P_{x}(x_{k})\geq 0$
$$\implies{} \sum\limits_{k=1}^{\infty} P_{x} (x_{k})=1.$$


\subsection{Properties of PMF's}
Now suppose we are given a PMF.\\
Then we should be able to calculate $P_{x}(k)$ for any $x$.\\
But also we cand find properties of these PMF's using the values of $P_{x}(k)$ we find.\\
For example

\subsubsection{The Weighted Sum}
We say
$$Mean~of~X=\sum\limits_{k} kP_{x}(k)$$
\noindent For the previous example \textbf{1.38}.
$$Mean~of~X=\sum\limits_{k} kP_{x}(k)$$
$$= 1* \frac{45}{81} + 2* \frac{27}{81} + 5* \frac{8}{81} + 10* \frac{1}{81}$$
$$= \frac{45+54+40+10}{81}$$
$$=\frac{109}{81}=1.8395.~~~\square$$


\noindent But what does this ``mean"\footnote{HAHAAAAHahaa haa haahaahaa ha... sorry}?\\

\noindent This is another form of what you may have learned for the mean in highschool, that being 
$$\frac{1}{n}\sum\limits_{i=1}^{n} x_{i}= \frac{x_{1}+x_{2}+\cdots+x_{n}}{n}$$
But in probability theory, we have a more sophistcated form of 
$$\frac{x_{1}+x_{2}+\cdots+x_{n}}{n}=\sum\limits_{k} kP_{x}(k)$$
since the probability measure of a random variable has n in the denominator. \\

But overall, to answer what does this ``mean", we can say that 
$x_{1}+x_{2}+\cdots+x_{n}$ is finite, but large (like the a series)
so in a limit process, i.e. over time, your over all average score will be (with arbitrarily many $n$ throws)
$$\frac{x_{1}+x_{2}+\cdots+x_{n}}{n} \xrightarrow{x\to \infty} 1.8395.$$











%%%----------------------------- Lecture 8: 2/6/23 
\subsection{Random Variables and \\PMF Examples}
\noindent \textbf{E.g. 1.17} An urn contains 4 red balls and 3 green balls. Two balls are drawn randomly. 
\begin{itemize} \item [(a)] Let $Z$ denote the number of green balls in the sample when the draws are done without replacement. Find the probability mass function of $Z$.\end{itemize}
\textit{Solution by Professor.}\\
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
	Z & 0 & 1 & 2\\
\hline
	$P_{Z}(k)$ & $\dfrac{4*3}{7*7}$ & $\dfrac{{4\choose1}{3\choose1}}{{7\choose2}}$ & $\dfrac{{4\choose0}{3\choose2}}{{7\choose2}}$ \\
\hline
\end{tabular}
\end{center}

\begin{itemize} \item [(b)] Let $W$ denote the number of green balls in the sample when the draws are donw with replacement. Find the probability mass function of $W$. \end{itemize}
\textit{Solution by Professor.}\\
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
	Z & 0 & 1 & 2\\
\hline
	$P_{W}(k)$ & $\dfrac{4*4}{7*7}$ & $\dfrac{4*3+3*4}{7*7}$ & $\dfrac{3*3}{7*7}$ \\
\hline
\end{tabular}
\end{center}

\noindent \textbf{E.g. 1.16} We flip a fair coin five times. For every heads you pay me \$1 and for every tails I pay you \$1. Let $X$ denote my net winnings at the end of five flips. Find the possible values and the probability mass function of $X$.\\
\textit{Solution.} ...can be found in the last hw, homework 1.5.~~~$\square$\\

\noindent \textbf{Exercise 1.19} You throw a dart and it lands uniformly a random on a circular dartboard of radius 6 inches. If your dart gets to within 2 inches of the center I will reward you with 5 dollars. But if your dart lands farther than 2 inches away from the center I will give you only 1 dollar. Let $X$ denote the amount of your reward in dollars. Find the possible values and the probability mass function of $X$.\\
\textit{Solution.} ...can be found in the last hw, homework 1.5.~~~$\square$





\chapter{Conditional Probability and Independence}
\section{Conditional Probability}
\textbf{The Definition of Conditional Probability}
\begin{center}
Let $A,B$ be events. The conditional probability of $A$ given $B$ has occured is defined by
$$P(A|B)=\frac{P(A\cap B)}{P(B)},~\text{if}~P(B)neq0.$$
\end{center}

\includegraphics[width=2in]{Conditional Prob Venn Diagram.png}
(Visually with a venn diagram, think about what probabilities you are dividng by, and why.)
\newpage


\noindent \textbf{E.g. 2.5} You have 4 reds and 6 greens. Choose 3 balls without replacement. What is the probability you get 2 reds, given that you've have at least 1 red. \\
\textit{Solution.}\footnote{(I do not know if I copied this down correctly.)}
\begin{align*}
& P(exactly~2~red~|~at~least~1~red)\\
&= \frac{P(exactly~2~red~\cap~at~least~1~red)}{at~least~1~red}\\
&= P\frac{exactly~2~red}{P(at~least~1~red)}\\
&= \frac{\dfrac{{4\choose 2}{6\choose1}}{{4\choose3}}}{1-P(no~red)}\\\\
&= \frac{\dfrac{{4\choose 2}{6\choose1}}{{4\choose3}}}{1-\dfrac{{4\choose 1}{6\choose2}}{{4\choose3}}}\cdots~~~\square
\end{align*}


\subsection{Multiplication Rule}
$$\boxed{P(A\cap B)=P(B)P(A|B)=P(A)P(B|A)}$$
\textit{Proof.} By the Definition of Conditional Probability, we have
$$P(A|B)=\frac{P(A\cap B)}{P(B)},~\text{if}~P(B)\neq0.$$ 
But of course,
$$P(B)P(A|B)=\frac{P(A\cap B)}{P(B)}P(B)\implies{}P(B)P(A|B)=P(A\cap B)$$
Therefore, $$P(A\cap B)=P(B)P(A|B)$$\hfill$q.e.d.$

\noindent \textbf{E.g.} You have 8 reds and 4 greens. Choose 2 balls without replacement. 
\begin{itemize}\item [(a)] $P(1st~red~|~2nd~red)=?$ \end{itemize}
\textit{Solution.}
\begin{align*}
& P(1st~red~|~2nd~red)\\\\
&= P(1st~red)P(2nd~red~|~1st~red)\\\\
&= \frac{8}{12} * \frac{7}{11}.~~~\square
\end{align*}

\begin{itemize}\item [(a)] $P(2nd~ball~is~red)=?$ \end{itemize}
\textit{Solution.}
\begin{align*}
& P(2nd~ball~is~red)\\\\
&= P(1st~red\cap 2nd~red)+P(1st~white\cap 2nd~red)\\\\
&= P(1st~R)P(2nd~R~|~1st~R)+P(1st~W)P(2nd~R~|~1st~W)\\\\
&= \frac{8}{12}* \frac{7}{11} + \frac{4}{12} * \frac{8}{11}.~~~\square
\end{align*}

\subsection{Law of Total Possibility}
\textbf{Theorem}\\
Let $\{B_1,B_2,B_3,...,B_n\}$ be a partition\footnote{Recall: A Partition is a set of sets such that it is a disjoint union of a set.
i.e. $\{B_1,B_2,B_3,...,B_n\}$ is called a partition of $\Omega$ if $\bigcup\limits_{i=1}^{n} B_{i}=\Omega$ and $B_{i}\cap B_{j}=\emptyset,~\forall~i\neq j.$} of the sample space $\Omega$. Then 
$$\boxed{P(A)=\sum\limits_{i=1}^{n} P(B_{i}\cap A)=\sum\limits_{i=1}^{n}P(B_{i})P(A\mid B_{i})}$$
Where the first equality is the partition, while the second equality is generalizing the conditional probability.




%%%-------------------------------- Lecture 9: 2/8/23
\subsection{Examples of Conditional Probability}
\textbf{Exercise 2.6} When Alice spends the day with the babysitter, there is a $0.6$ probability that she turns on the TV and watches a show. Her little sister Betty cannot turn on the TV on by herself. But once the TV is on, Betty watches with probability $0.8$. Tomorrow the girls spend the day with the babysitter.\\

% https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBoAmAXVJADcBDAGwFcYkQQBfU9TXfQigCMpAMzU6TVuwAqANQB+AeQBiKrjxAZseAkRFCJDFm0Qh5ywt146BRcqWJGppkAEFGWAMYwFAdXocLwALeA0bfj0UByoaY2kzACEYHBwATwACAHdAkLDOCRgoAHMwlFAAMwAnCABbJDIQHAgkERBGegAjGEYABT5dQRAsMGxYEDiXdl6AXmIAOgAWcJBquoaaZqQHdq6e-tso4dGsccmTabn5gDYVtfrEHa3EURoO7r6BuzMRsbZzhIgWZCO41B5PFqIRZvPafQ5DX6nf6SC5mWYLAAcXEonCAA
\begin{tikzcd}
                                                                   &                                                                     & Alice~Watches \\
                                                                   & TV~On \arrow[ru, "P=1" description] \arrow[rd, "P=0.8" description] &               \\
{} \arrow[rd, "P=0.4" description] \arrow[ru, "P=0.6" description] &                                                                     & Betty~ watches \\
                                                                   & TV~OFF                                                              &              
\end{tikzcd}



\begin{itemize} \item [(a)] What is the probability that both Alice and Betty watch TV tomorrow? \end{itemize}
\textit{Solution.}
\begin{align*}
&P(Alice~watches~\cap~Betty~watches)\\
&=P(TV~on)P(Alice~and~Betty~watches~|~TV~on)\\
&=(0.6)(1)(0.8)=(0.6)(0.8)=0.48~.~~~\square
\end{align*}

\begin{itemize} \item [(b)] What is the probability that only Betty watches TV tomorrow? \end{itemize}
\textit{Solution.}
$(0.6)(0.8)=0.48~.~~~\square$

\begin{itemize} \item [(c)] What is the probability that only Alice watches TV tomorrow? \end{itemize}
\textit{Solution.}
$(0.6)(0.2)=0.12~.~~~\square$\\

\noindent \textbf{Exercise 2.3} \\
$P($a randomly chosen number between 1 and 100 is divisible by 3 $|$ the number has at least 1 digit equal to 5$)$\\
\textit{Solution.}\\
$P=P(A|B)=\dfrac{P(A\cap B)}{P(B)}$\\
$B=\{5,15,25,35,45,55,65,75,85,95,$\\
$50,51,52,53,54,56,57,58,59\}$\\

\noindent and $A\cap B = \{15,51,45,54,57,75\}$
$$\dfrac{\frac{6}{100}}{\frac{19}{100}}=\dfrac{6}{19}.~~~\square$$

\newpage
\noindent \textbf{Exercise 2.3} From 52 cards deal three cards without replacement. Find $P(1st~card~is~a~queen,~2nd~is~a~king,~3rd~is~an~ace)$.\\

\noindent \textit{Solution 1.} \\
Intuitively, 
$$P(1st~Q,2nd~K,3rd~A)=\dfrac{4}{54} * \dfrac{4}{51} * \dfrac{4}{50}.~~~\square$$\\

\noindent \textit{Solution 2.} \\
More mathematically,\\
$$P(A\cap B \cap C) = P(A)~P(B|A)~P(C|A\cap B)$$ \begin{center}(using multiplication rule)\end{center}
$$=\dfrac{4}{52} * \dfrac{4}{51} * \dfrac{4}{50}.~~~\square$$





\section{Bayes' Formula}
\textbf{E.g.} Say you have two boxes, \\
Box 1 := $\{2g,1r\}$\\
Box 1 := $\{2g,3r\}$\\

\noindent Suppose we choose a box randomly,
then we draw a ball from the selected box.\\
If the sample ball is $r$, what is the conditional probability 
that the ball comes from box 1.
\newpage
\noindent \textit{Solution.}\\
The probability that you get a box 1 or 2 is both $\frac{1}{2}$.\\
For box 1, getting red has probability $\frac{1}{3}$.\\
\noindent For box 2, getting red has probability $\frac{3}{5}$.\\
\begin{align*}
&P(Box~1~|~r)\\
&=\dfrac{P(Box~1~\cap~r)}{P(r)}\\
&=\dfrac{P(Box~1~\cap~r)}{P(Box~1~\cap~r)+P(Box~2~\cap~r)}\\
&=\dfrac{P(Box~1)P(r~|~Box~1)}{P(Box~1)P(r~|~Box~1)+P(Box~2)P(r~|~Box~2)}\\
&=\dfrac{\frac{1}{2}* \frac{1}{3}}{\frac{1}{2} * \frac{1}{3} + \frac{1}{2} * \frac{3}{5}}\\
&=\dfrac{\frac{1}{3}}{\frac{1}{3} + \frac{3}{5}}\\
&=\dfrac{\dfrac{1}{3}}{\frac{5+9}{15}}=\dfrac{1}{3}* \dfrac{15}{14}=\dfrac{5}{14}.~~~\square
\end{align*}

\noindent \textit{Shall we try to generalize this?}\\
\noindent Say we know, \\
\begin{center}
% https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBoAmAXVJADcBDAGwFcYkQQBfU9TXfQinKli1Ok1bsAQgH0AjFx4gM2PASLCqNBizaIQs8ot6qBG0gBYxOyftkBmY8r5rByCxWsS9IAIJcxGCgAc3giUAAzACcIAFskMhAcCCQ5GkZ6ACMYRgAFFzN9LDBsWBBtb3ZcgApZOQBKJ2i4hJpkpGEQDOy8gvUikqwyit0q2plyRu5ImPjERPbEe3SsnPzTfpBi0rYR2xAahymlZrm0pJTEDy7V3o3BLcHh8VH9Gt8AHzrjmZbETsW126az6D22Q12L327y+Ex+IFOSGWFyQQNu634m3BzxsPhhRwCnCAA
\begin{tikzcd}
                                                                                                          &  & B_1 \arrow[rrdd, "P(A|B_1)" description] &  &   \\
                                                                                                          &  &                                          &  &   \\
{} \arrow[rruu, "P(B_1)" description] \arrow[rr, "P(B_2)" description] \arrow[rrdd, "P(B_3)" description] &  & B_2 \arrow[rr, "P(A|B_2)" description]   &  & A \\
                                                                                                          &  &                                          &  &   \\
                                                                                                          &  & B_3 \arrow[rruu, "P(A|B_3)" description] &  &  
\end{tikzcd}
\end{center}

\noindent \textbf{Definition. 1} Where we call $P(B_1),P(B_2),...,P(B_n)$ the ``Prior Probabilities ".\\
\textbf{Definition. 2} Where we call $P(B_1),P(B_2),...,P(B_n)$ the ``Posterior Probabilities ".\\

\noindent Of course, \textbf{Main Question:} $P(B_i | A)=?$\\
i.e. whats the probability going backwards? \\




%%%------------------------------- Lecture 10: 2/13/23
\noindent\textbf{Theorem.}\\
Let $B_{1},B_{2},B_{3},...,B_{n}$ be a partition of the smaple space $\Omega$. 
Then $$\boxed{P(B_{i}|A)=\dfrac{P(B_{i})P(A|B_{i})}{\sum\limits_{j=1}^{n}P(B_{j})P(A|B_{j})},~\forall~i\in\mathbb{N}}$$\\
\textit{General procedure:}\\
Quickly remark, $j$ is ``all" the $B_j$'s, while $i$ is the one we want to find (``the specific $B_i$").\\
--- Calculate $P(A)$. But notice that\\
--- $P(A)=P(B_1)P(A|B_1)+P(B_2)P(A|B_2)+\cdots+P(B_j)P(A|B_j)$ by the sum of multiplication rules (instead of writting out all the intersection definitions\footnote{however, the professor may ask you to write/prove this for your question, in which case you have to write everything.}, for $j\in\mathbb{N}$.\\
--- This is exactly what is on the bottom of the theorem, i.e. $P(A)=\sum\limits_{j=1}^{n} P(B_{j})P(A|B_{j})$.\\
--- Now all one needs to do is find the specific $i$th conditional probability, $P(B_{i})P(A|B_{i})$, where $1<i<j$. Or you could just recalculate it. \\
--- Then finally calculate $\boxed{P(B_i)=\frac{P(B_i)P(A|B_i)}{P(A)}}$. This is since,  \newpage
$$\boxed{P(B_i|A)=\frac{P(B_{i})P(A|B_{i})}{P(A)}}$$
$$=\frac{P(B_{i})P(A|B_{i})}{P(B_1)P(A|B_1)+P(B_2)P(A|B_2)+\cdots+P(B_j)P(A|B_j)}$$\\
$$=P(B_{i}|A)=\dfrac{P(B_{i})P(A|B_{i})}{\sum\limits_{j=1}^{n}P(B_{j})P(A|B_{j})}$$\\
which is exactly our theorem, except simplified.



%%%--------------------------------- Lecture 11: 2/15/23
\subsection{Weird Observations of\\ Bayes' Theorem}
Of course, this is not difficult\footnote{seriously, I'm not lying, I think...} after a few examples.\\
\textbf{E.g.} Say the probability of picking either box is equal.
\begin{itemize}
\item Box 1 := $\{2g,1r\}$\\
\item Box 2 := $\{3g,5r\}$\\
\end{itemize}
\noindent Say we randomly choose one box.\\
Then we draw a ball randomly from the selected box.\\
If the sample ball is $r$, what is the conditional probability 
that the ball comes from box \#2.
\newpage
\noindent \textit{Proof Style. (like above)}\\
\begin{align*}
&P(Box~2~|~r)=\dfrac{P(Box~2~\cap~r)}{P(r)} \\\\
&=\dfrac{P(Box~2~\cap~r)}{P(Box~2~\cap~r)+P(Box~2~\cap~r)} \\\\
&=\dfrac{P(Box~2)P(r~|~Box~2)}{P(Box~1)P(r~|~Box~1)+P(Box~2)P(r~|~Box~2)}\\
&=\dfrac{\frac{1}{2}* \frac{5}{8}}{\frac{1}{2} * \frac{1}{3} + \frac{1}{2} * \frac{5}{8}}\\
&=\dfrac{\frac{5}{16}}{\frac{1}{6}\left(\frac{8}{8}\right) + \frac{5}{16}\left(\frac{3}{3}\right)}\tag{just making common denominators}\\
&=\dfrac{\frac{5}{16}}{\frac{8+15}{48}}=\frac{5}{\cancel{16}}* \dfrac{\cancelto{3}{48}}{23}=\dfrac{15}{23}.
\end{align*}

\hfill $q.e.d.$\\

\noindent \textit{Better Workflow Solution.}\\
We know:
\begin{itemize}
\item $P(B_1)=1/2$
\item $P(B_2)=1/2$
\end{itemize}
We need to find:
\begin{enumerate}
\item $P(r)=?$, and remember conditional of $B_2$
\item $\dfrac{conditional~of~B_2}{P(r)}$
\end{enumerate}
Now,
\begin{align*}
& P(r)=P(B_1)P(r|B_1)+\boxed{P(B_2)P(r|B_2)}   \tag{$\boxed{\text{remember}}$}\\
& = \left( \frac{1}{2} \right) \frac{1}{3} + \boxed{\left( \frac{1}{2} \right) \frac{5}{8}}\\
& = \frac{1}{6} + \boxed{\frac{5}{16}} = \frac{8}{48} + \boxed{\frac{15}{48}} = \mathbf{\frac{23}{48}}
\end{align*}
And finally
\begin{align*}
& P(B_2|r)=\frac{\boxed{P(B_2)P(r|B_2)}}{\mathbf{P(B_1)P(r|B_1)+P(B_2)P(r|B_2)}}   \\
& = \frac{\left( \frac{1}{2} \right) \frac{5}{8}}{ \left( \frac{1}{2} \right) \frac{1}{3} + \left( \frac{1}{2} \right) \frac{5}{8}}\\
& = \frac{\frac{5}{16}}{\frac{23}{48}} = \boxed{\frac{5}{16}}\mathbf{\left(\frac{48}{23}\right)}=\frac{15}{23}.~~~\square
\end{align*}
\textit{Remark:} $\frac{15}{23}>\frac{1}{2}$. Or in english, this probability is \textbf{higher} than the prior probability. The probability that you chose the 2nd box if you have a red ball is higher than the probability you choose a random box. Which should make sense to you!\footnote{I hope...}

\textbf{E.g. 2.14} Suppose we have a medical test that detects a particular disease 96\% of the time, but gives false positive 2\% of the time. Assume that 0.5\% of the population carries the disease. If a random person test positive for the disease, what is the probability that they actually carry the disease?\\

This is a typical example of using bayes' theorem. I will use the \textit{Better Workflow Solution.} but even faster.\\

\newpage
\noindent \hypertarget{better workflow solution faster}{\textit{Better Workflow Solution (Faster).}} \\
Let $D$ be the event ``you are diseased"\footnote{is this even a word?}, and the $\boxed{\pm}$ is tested positive or negative.\\
The question asks, $P(D)=?$\\
A Quick Sketch:\\
\begin{center}
% https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBoBGAXVJADcBDAGwFcYkQQBfU9TXfQigBMpYtTpNW7ACJceIDNjwEiIoeIYs2iENIB6AYTm8lAogBZRGydpAAdOwCMIADxhRgAak7GFfZYLIluo0mlI6Ds5uHgC0Ppzi7gDm8ESgAGYAThAAtkhkIDgQSOQ0jPSOMIwACv5mOlhg2LAgoTbsAHTExACsvlm5+TRFSCIg5ZU1dSoNTVgtbVqdAJzLfdwZ2XmIpYXFiADMZRVVtaYzII3NbIvhIB3LAGz9W6PD+5bjJ1PngpdzCwkSx0AH4XoNEGMRodjpMzvwLld5jcgXcukJWiA4AALLDpHBDcaNWxwCCMeaYypgKBIGIHYgbEADba7aGfMK2ME0Kk0w4FCanaZ-JEtBKcIA
\begin{tikzcd}
                                                                   &  & D \arrow[rr, ".96" description] \arrow[rrdd, "?" description, bend left]    &  & \boxed{+} \\
{} \arrow[rru, ".005" description] \arrow[rrd, ".995" description] &  &                                                                             &  &           \\
                                                                   &  & D^C \arrow[rr, "?" description] \arrow[rruu, ".02" description, bend right] &  & \boxed{-}
\end{tikzcd}
\end{center}
\begin{align*}
&P(D|+)\implies{\text{~find~}}P(+|D)P(D) ~\land~P(+)	\\
&\text{thinking swap D,+}
\end{align*}
Now,
\begin{align*}
P(+)&=\boxed{P(D)P(+|D)}+P(D^C)P(+|D^C)\\
&=\boxed{.005(.96)}+.995(.02)=\boxed{.0048}+.0199\\
&=\mathbf{.0247}
\end{align*}
And finally,
\begin{align*}
P(D|+)&=\frac{\boxed{P(D)P(+|D)}}{\mathbf{P(D)P(+|D)+P(D^C)P(+|D^C)}}\\
&=\frac{\boxed{.0048}}{\mathbf{0.247}}=.194~~~\square
\end{align*}
\newpage
\subsubsection{Some Remarks of E.g. 2.14}
Observe the ``prior probabilities" are
\begin{itemize}
\item $P(D)=.005$
\item $P(D^C)=.995$
\end{itemize}
And the ``posterior probabilities" are
\begin{itemize}
\item $P(+|D)=.96$
\item $P(+|D^C)=.02$
\item $P(-|D)=?$
\item $P(-|D^C)=?$
\end{itemize}
Notice that we were missing some of these, but it didnt matter since we were trying to find the conditional probability of the first, i.e. $P(D|+)$. Which is easy to see with the tree diagram. 
\begin{center}
% https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBoBGAXVJADcBDAGwFcYkQQBfU9TXfQigBMpYtTpNW7ACJceIDNjwEiIoeIYs2iENIB6AYTm8lAogBZRGydpAAdOwCMIADxhRgAak7GFfZYLIluo0mlI6Ds5uHgC0Ppzi7gDm8ESgAGYAThAAtkhkIDgQSOQ0jPSOMIwACv5mOlhg2LAgoTbsAHTExACsvlm5+TRFSCIg5ZU1dSoNTVgtbVqdAJzLfdwZ2XmIpYXFiADMZRVVtaYzII3NbIvhIB3LAGz9W6PD+5bjJ1PngpdzCwkSx0AH5WuNGrYoBAcDh3C9BogxiNDsdJmd+BcrvMbkC7l0hOC4AALLDpHBDCFgWxwCCMebgypgKBIGIHYgbEADba7FGfMK2ME0Jksw4FCanaZ-bGA+nU9jQ2HwhKcIA
\begin{tikzcd}
                                                                   &  & D \arrow[rr, ".96" description] \arrow[rrdd, "?" description, dotted, bend left]    &  & \boxed{+} \\
{} \arrow[rru, ".005" description] \arrow[rrd, ".995" description] &  &                                                                                     &  &           \\
                                                                   &  & D^C \arrow[rr, "?" description, dotted] \arrow[rruu, ".02" description, bend right] &  & \boxed{-}
\end{tikzcd}
\end{center}
In fact, here it is simplified:
\begin{center}
% https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBoBGAXVJADcBDAGwFcYkQQBfU9TXfQigBMpYtTpNW7ACJceIDNjwEiIoeIYs2iENIB6AYTm8lAogBZRGydpAAdOwCMIADxhRgAak7GFfZYLIluo0mlI6Ds5uHgC0Ppzi7gDm8ESgAGYAThAAtkhkIDgQSOQ0jPSOMIwACv5mOlhg2LAgoTbsAHTExACsvlm5+TRFSCIg5ZU1dSoNTVgtbVqdAJzLfdwZ2XmIpYXFiADMZRVVtaYzII3NbIvhIB3LAGz9W6PD+0fjJ1PngpdzCwkSx0XSErRAcAAFlh0jghuNGrY4BBGPNwZUwFAkDEDsQEpwgA
\begin{tikzcd}
                                                                   &  & D \arrow[rr, ".96" description]                 &  & \boxed{+} \\
{} \arrow[rru, ".005" description] \arrow[rrd, ".995" description] &  &                                                 &  &           \\
                                                                   &  & D^C \arrow[rruu, ".02" description, bend right] &  & \boxed{-}
\end{tikzcd}
\end{center}
which is the obviously just:
\begin{center}
% https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBoBGAXVJADcBDAGwFcYkQQBfU9TXfQigBMpYtTpNW7ACJceIDNjwEiIoeIYs2iENIB6AYTm8lAogBYKGydpAAdOwCMIADxhRgAak5dx7gObwRKAAZgBOEAC2SGQgOBBI5DSM9I4wjAAKfMqCIFhg2LAgNJpSOgB0xMQArMYg4VExNPFIIiApaZnZZjr5hWwlNuzlAJwjtdyhEdGISXEJiADMyanpWaYqvQVYRYNawyMAbHUNM20tSyud6-ybedu7EvsVxELFIHAAFlghOE3t+VscAgjB27zSYCgMU4lE4QA
\begin{tikzcd}
                                                                   &  & D \arrow[rrd, ".96" description]   &  &           \\
{} \arrow[rru, ".005" description] \arrow[rrd, ".995" description] &  &                                    &  & \boxed{+} \\
                                                                   &  & D^C \arrow[rru, ".02" description] &  &          
\end{tikzcd}
\end{center}
Also notice our final value, $P(D|+)=\boxed{.194}$~.\\

\noindent \textbf{This seems very low.}
\textit{INSERT YOUR CONCERNED FACE.}\\
Anyways, this is pretty counterintuitive, but there's an interesting ``philosophical" technicality. If you notice the prior probabilies, you can think of this (technically) as the probability that a random chosen person (from the side of the road or something) is sick. This implies, that the probability that they have a false positive is pretty high, since you don't know if they were sick or not. Or you can alternatively think the false positive rate is high since there is a very high chance they aren't actually sick. \\

Thinking like this should be similar to saying, there are more people that are sick that still test positive.

At least compared to those who are sick, even though they almost always test positive.

\subsubsection{\textbf{``Unraveling the Tree"}}
\textit{I discovered this technique,}\footnote{I didn't search this up, but it has probably been invented somewhere. If so, I do not take credit, but it be cool if I found new math more often...} which is useful in checking if my answer makes sense.\newpage
\textbf{Claim: }\textit{Depending on the products of each conditional probability, one of the ``branches" have a higher probability than the other.}\\
Visually, one should think like this:
\begin{center}
% https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBoBGAXVJADcBDAGwFcYkQQBfU9TXfQigBMpYtTpNW7AFRceIDNjwEiIoeIYs2iELO68lAogBZRGydpAA6YsWMAOOQf4qUp9TU1SdN8gE4-JwU+ZUFkUypPC3YAHRjGejAoLnEYKABzeCJQADMAJwgAWyQyEBwIJHIaBIAjGEYABRCjHSwwbFgQKK12G2IAVi6QAAsYemSdSDA2fRB8opKacqQREFr6psNXEDaOtm7vawDBmlHx9imZ+XnixCqyisQAZmr6Osbm7d2sToPLKz8ADYgjcVktHqY1m8Np9BDt2j99hIej5iEIUpwgA
\begin{tikzcd}
                                                                                     &  & * \arrow[rr, ".96" description] &  & .0048 \\
{} \arrow[rru, ".005" description, no head] \arrow[rrd, ".995" description, no head] &  &                                 &  & \land \\
                                                                                     &  & * \arrow[rr, ".02" description] &  & .0199
\end{tikzcd}
\end{center}

This does have its uses, since your just computing the conditional probabilities ahead of time. 

Let's actually modify \textbf{Ex. 2.14} to make the final value less concerning, I will also use this new method to calculate this even faster, then the faster version, of the\\
\textit{Better Workflow Solution.}\footnote{you don't believe me, do you...}\\

\textbf{E.g. 2.14 (Modified)} Let the posterior probabilites, i.e. the probability that you are sick or not, be equal. \\

Why? This number \textbf{is} out of thin air (that is $P(D)=P(D^C)=1/2$, but the idea is that ``if you are ate the doctors, you are probably half as likely to be sick or not". But to step away from making this too philosophical, (going back to math) this basically just means the posterior probabilities are normalized, so the only thing that changes final answer is the conditional probabilies. \\

(i.e. if you test positive, you are definitely sick \scriptsize{(at least .96 of the time)} \normalsize and vice versa)
\newpage
\noindent \textit{Enough Rambling Dean, Solution by rushing through Procrastination.}\\
\begin{center}
% https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBoBGAXVJADcBDAGwFcYkQQBfU9TXfQigBMpYtTpNW7ACJceIDNjwEiIoeIYs2iENIB6AYTm8lAogBZRGydpAAdOwCMIADxhRgAak7GFfZYLIluo0mlI6Ds5uHgC0Ppzi7gDm8ESgAGYAThAAtkhkIDgQSOQ0jPSOMIwACv5mOlhg2LAgoTbsAHTExACsvlm5+TRFSCIg5ZU1dSoNTVgtbVqdAJzLfdwZ2XmIpYXFiADMZRVVtaYzII3NbIvhIB3LAGz9W6PD+5bjJ1PngpdzCwkSx0AH4XoNEGMRodjpMzvwLld5jcgXcukJWiA4AALLDpHBDcaNWxwCCMeaYypgKBIGIHYgbEADba7aGfMK2ME0Kk0w4FCanaZ-JEtBKcIA
\begin{tikzcd}
                                                                   &  & D \arrow[rr, ".96" description] \arrow[rrdd, "?" description, bend left]    &  & \boxed{+} \\
{} \arrow[rru, ".5" description] \arrow[rrd, ".5" description] &  &                                                                             &  &           \\
                                                                   &  & D^C \arrow[rr, "?" description] \arrow[rruu, ".02" description, bend right] &  & \boxed{-}
\end{tikzcd}
\end{center}
$$\parallel$$
\begin{center}
% https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBoBGAXVJADcBDAGwFcYkQQBfU9TXfQigBMpYtTpNW7ACJceIDNjwEiIoeIYs2iENIB6AYTm8lAogBYKGydpAAdOwCMIADxhRgAak5dx7gObwRKAAZgBOEAC2SGQgOBBI5DSM9I4wjAAKfMqCIFhg2LAgNJpSOgB0xMQArMYg4VExNPFIIiApaZnZZjr5hWwlNuzlAJwjtdyhEdGISXEJiADMyanpWaYqvQVYRYNawyMAbHUNM20tSyud6-ybedu7EvsVxELFIHAAFlghOE3t+VscAgjB27zSYCgMU4lE4QA
\begin{tikzcd}
                                                                   &  & D \arrow[rrd, ".96" description]   &  &           \\
{} \arrow[rru, ".5" description] \arrow[rrd, ".5" description] &  &                                    &  & \boxed{+} \\
                                                                   &  & D^C \arrow[rru, ".02" description] &  &          
\end{tikzcd}
\end{center}
$$\parallel$$
\begin{center}
% https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBoBGAXVJADcBDAGwFcYkQQBfU9TXfQinKli1Ok1bsAVFx4gM2PASLCATGIYs2iEDO69FAoqpEaJ2kAB1LAIwgAPGFGAA6ACwAOTrIP9lKE3UaTUkdF2JyH3k+JUFkAGYKMy12azhmAFsohT84tyTg81TLDPocAAsbADNXNwBOb31ow39kEypClJ1rRggAJy4xJwBzeCJQKr6ILMQyEBwIJGEQRnobGEYABRijHSwwbFgQTtCQcOIAVmOQcph6KHZIMDYmyemkOYWkExW1je2WoIQPtDmwThYXHU6lcaLd7o8CC85G8ZssvohEr91lsdv5gQcsEdwexIQA2KIo740dH5LH-XFAkGEsHiLpnYiqClTGaY9EXV7cpC0vkC96IC7UxaIUmcSicIA
\begin{tikzcd}
                                                                                   & * \arrow[r, ".96" description] & \boxed{.48} \arrow[rd] &                &              \\
{} \arrow[ru, ".005" description, no head] \arrow[rd, ".995" description, no head] &                                & \lor                   & \sum \arrow[r] & \mathbf{.49} \\
                                                                                   & * \arrow[r, ".02" description] & .01 \arrow[ru]         &                &             
\end{tikzcd}
\end{center}
The $.48>.01$ is not important, but good to know, and it makes sense. Now,
$P(D|+)=\dfrac{\boxed{.48}}{\mathbf{.49}}=.979~~~\square$
\footnote{See told you so! But also you should probably do it \hyperlink{better workflow solution faster}{\textit{Better Workflow Solution (Faster).}} way for the professor to give most credit. Still, as I said, this useful to check work.}

\noindent \textit{Final Remark} 
Yes! This value makes sense in this context. The average person will most likely test positive, assuming that they went to the doctors ant have a half chance of being sick in the first place...ok i'll shut up.







\section{Independence}\footnote{somthing I don't believe I have even though im 20 (at the time of writting this book)}
\textbf{Definition.} Events $A$ and $B$ are independent if 
$$\boxed{P(A\cap B)=P(A)P(B)}$$

\textbf{E.g.} Suppose we have a bag with 2 green and 3 red. Choose 2 balls with replacement. $P(1st~ red \cap 2rd ~red)$\\
\textit{Solution} \\
Since the sample size stays the same (with replacement), the events are independence.\\
$P(1st~ red \cap 2rd ~red)=P(1st~red)P(2nd~red)$\\
$=\frac{3}{5} * \frac{3}{5}=\frac{9}{15}$\\

\textbf{Good question from another student:} Whats the difference between this and disjoint events?


well disjoint events you can represent and draw (with venn diagram).
You cannot represent independent events.

%---
%
%If $A$ and $B$ are disjoint and independent, then 
%$$P(A)P(B)=P(A\cap B)=P(\emptyset)=0$$
%$\implies P(A)=0$ or $P(B)=0$.
%
%
%---
%### E.g. Suppose we toss a coin and roll a die.
%$A=\{coin~H\}$
%$B=\{die~4\}$
%
%$P(A\cap B)=P(A)P(B)=\dfrac{1}{2} * \dfrac{1}{6} = \dfrac{1}{12}$
%
%
%graph pmf, see each cartesian set from the cartesian product only constitute one of the axis, i.e. one of the full range of a set, and one single event from the other.....
%
%---
%## Consequences of independence
%
%If $A$ and $B$ are independend and $P(B)>0$, 
%then $P(A|B)=P(A)$
%
%$Proof:$ 
%$LHS = P(A|B)$ and by def^n
%$=\frac{P(A\cap B)}{P(B)}$ and by independence
%$=\frac{P(A)\cancel{P(B)}}{\cancel{P(B)}}$
%$=P(A)=RHS$
%
%
%---
%## Theorem 2.20 - If $A$ and $B$ are independent, then
%### $A$ and $B^{C}$ are independent.
%### $A^{C}$ and $B$ are independent.
%### $A^{C}$ and $B^{C}$ are independent.
%
%Proof (a) Suppose $A$ and $B$ are independent. 
%$P(A\cap B^{C})=P(A)-P(A\cap B)=$ and by independence
%$=P(A)-P(A)P(B)=P(A)[1-P(B)]=P(A)P(B^{C})\implies$
%$A and B^{C}$ are independent.
%
%
%---
%## Pairwise Independence
%$DEF^n:$ $A,B,$ and $C$ are independent if 
%$$
%\begin{align*}\\
%&P(A\cap B) = P(A)P(B) \\
%&P(A\cap C) = P(A)P(C) \\
%&P(B\cap C) = P(B)P(C) 
%\end{align*}
%$$
%
%and 
%
%$$P(A\cap B \cap C)=P(A)P(B)P(C)$$
%
%
%both needed, not one implies other...
%
%---
%### E.g. $(2) \cancel\implies (1)$
%Chose a random number from $[0,1]$.
%Let $A=[\frac{1}{2},1]\implies P(A)=\frac{1}{2}$
%$B=[\frac{1}{2},\frac{3}{4}]\implies P(B)=\frac{1}{4}$
%$C=[\frac{1}{16},\frac{9}{16}]\implies P(B)=\frac{1}{2}$
%
%$$P(A\cap B \cap C)=\frac{1}{16} = P(A)P(B)P(C)$$







































































\backmatter
\begin{thebibliography}{99}
\bibitem{Introduction to Probability}
Anderson, David F., et al. \emph{Introduction to Probability}, Cambridge University Press (1991) \\
ISBN: 978-1-108-41585-9
\end{thebibliography}
\end{document}
